# Babylon Architecture Decisions
# Key decisions and their rationale (ADR-style)

meta:
  version: "1.10.0"
  updated: "2026-01-01"
  format: "decision_id: {status, date, context, decision, rationale, consequences}"

# =============================================================================
# ARCHITECTURAL DECISIONS
# =============================================================================

decisions:

  ADR001_embedded_trinity:
    status: "accepted"
    date: "2024-12"
    title: "Three-layer local architecture (Ledger/Topology/Archive)"
    context: |
      Game needs to handle rigid state (economics), fluid state (relationships),
      and semantic memory (history/narrative) differently.
    decision: |
      Separate into three architectural layers:
      - Ledger: SQLite/Pydantic for material state
      - Topology: NetworkX for relational state
      - Archive: ChromaDB for semantic memory
    rationale:
      - "Each layer has different consistency/mutability requirements"
      - "Separation enables independent testing and evolution"
      - "Mirrors materialist distinction: base/relations/superstructure"
    consequences:
      positive:
        - "Clear separation of concerns"
        - "Can swap implementations per layer"
      negative:
        - "More complexity than single-store approach"
        - "Cross-layer queries require coordination"

  ADR002_sqlite_over_postgres:
    status: "accepted"
    date: "2024-12-07"
    title: "SQLite instead of PostgreSQL"
    context: |
      Initially configured PostgreSQL, but this is a local game
      with no multi-user requirements.
    decision: "Use SQLite for all persistent storage"
    rationale:
      - "No server to manage"
      - "Zero configuration"
      - "Portable game saves (single file)"
      - "Sufficient performance for single-player"
    consequences:
      positive:
        - "Simpler deployment"
        - "No external dependencies"
      negative:
        - "Can't easily add multiplayer later"
        - "No concurrent writes (not needed)"

  ADR003_ai_as_observer:
    status: "accepted"
    date: "2024-12"
    title: "AI generates narrative, never controls mechanics"
    context: |
      LLMs are non-deterministic. Game mechanics must be
      reproducible and testable.
    decision: |
      AI sits in Archive layer only. It can:
      - Query state and history
      - Generate narrative text
      It cannot:
      - Modify game state
      - Determine mechanical outcomes
    rationale:
      - "Deterministic mechanics enable testing"
      - "Reproducible gameplay"
      - "AI failures don't break game"
      - "Separation of narration from simulation"
    consequences:
      positive:
        - "Game works without AI"
        - "Testable mechanics"
      negative:
        - "Less emergent AI behavior"
        - "More explicit mechanics required"

  ADR004_json_over_xml:
    status: "accepted"
    date: "2024-12-07"
    title: "JSON for game data instead of XML"
    context: |
      Legacy data was in XML. Need machine-readable format
      that's easy to validate and work with.
    decision: "Migrate all game data to JSON with JSON Schema validation"
    rationale:
      - "Native Python support (json module)"
      - "JSON Schema provides validation"
      - "Easier to read/write programmatically"
      - "Better tooling ecosystem"
    consequences:
      positive:
        - "Schema validation catches errors early"
        - "Cleaner data structures"
      negative:
        - "Migration effort (completed)"
        - "XML kept as reference (minor storage cost)"

  ADR005_json_schema_2020:
    status: "accepted"
    date: "2024-12-07"
    title: "JSON Schema Draft 2020-12"
    context: |
      Need schema validation for game data. Multiple JSON Schema
      versions exist.
    decision: "Use Draft 2020-12, the latest stable version"
    rationale:
      - "Best $ref resolution"
      - "Dynamic references support"
      - "Active development"
      - "referencing library support"
    consequences:
      positive:
        - "Modern features available"
        - "Good library support"
      negative:
        - "Some older tools may not support"

  ADR006_pydantic_models:
    status: "accepted"
    date: "2024-12"
    title: "Pydantic for all game objects"
    context: |
      Need runtime validation and type safety for game entities.
    decision: "All game objects are Pydantic BaseModel subclasses"
    rationale:
      - "Runtime validation"
      - "Automatic serialization"
      - "IDE support"
      - "Clear schema definition"
    consequences:
      positive:
        - "Type safety"
        - "Self-documenting models"
      negative:
        - "Slight performance overhead"
        - "Learning curve"

  ADR007_data_driven_design:
    status: "accepted"
    date: "2024-12"
    title: "Game logic in data, not code"
    context: |
      Game has many entities with varying behaviors. Hardcoding
      each case leads to unmaintainable code.
    decision: |
      Define game rules in data files (JSON/TOML).
      Code provides engine, data provides content.
    rationale:
      - "Easy content modification"
      - "Modding support"
      - "Clear separation of concerns"
      - "Non-programmers can adjust balance"
    consequences:
      positive:
        - "Flexible content"
        - "Moddable"
      negative:
        - "More complex data structures"
        - "Data validation critical"

  ADR008_test_separation:
    status: "accepted"
    date: "2024-12"
    title: "Separate fast tests from AI tests"
    context: |
      AI tests are slow and non-deterministic.
      Logic tests should be fast and reliable.
    decision: |
      Use pytest markers to separate:
      - math, ledger, topology: fast, deterministic
      - ai: slow, can be flaky
    rationale:
      - "Fast feedback loop for logic changes"
      - "CI can run fast tests on every commit"
      - "AI tests run less frequently"
    consequences:
      positive:
        - "Fast TDD cycle"
        - "Clear test categories"
      negative:
        - "Must remember to run both"

  ADR009_networkx_topology:
    status: "accepted"
    date: "2024-12-07"
    title: "NetworkX for Topology layer graph storage"
    context: |
      The Embedded Trinity architecture (ADR001) specifies a Topology layer for
      fluid relational state: class solidarity networks, dialectical relationships,
      supply chains, tension flows. This requires a graph data structure.

      Options evaluated:
      - NetworkX: Pure Python, rich algorithms, in-memory
      - igraph: C-based, faster, less Pythonic API
      - graph-tool: C++ core, fastest, complex installation
      - Neo4j: Full graph DB, requires external server
      - SQLite edge tables: DIY graph, no built-in algorithms
      - RDFLib: Semantic web focus, overkill for game state
    decision: |
      Use NetworkX as the graph engine for the Topology layer.
      Graph state persists to SQLite (Ledger) for save/load.
      At startup, build NetworkX graph from SQLite edge data.
      During gameplay, mutate graph in-memory.
      On save, serialize graph back to SQLite.
    rationale:
      - "Pure Python: no external servers, matches 'no external dependencies' principle"
      - "Rich algorithm library: centrality, paths, community detection built-in"
      - "Sufficient performance: game will have ~1000-10000 nodes max, not millions"
      - "Pythonic API: easy to test, integrates well with Pydantic models"
      - "Serializable: nx.node_link_data() exports to JSON for SQLite storage"
      - "Phase 1 scope: 2 nodes, 1 edge - any graph library works, pick simplest"
      - "TDD-friendly: graph operations are pure functions, easy to unit test"
    alternatives_rejected:
      igraph:
        reason: "C dependency complicates installation; performance gain unnecessary at our scale"
      graph_tool:
        reason: "Difficult to install (needs C++ compilation); overkill for game-sized graphs"
      neo4j:
        reason: "Violates 'no external servers' principle; adds operational complexity"
      sqlite_edges:
        reason: "Would require implementing graph algorithms from scratch; NetworkX provides them"
    consequences:
      positive:
        - "Zero external dependencies beyond Python package"
        - "Rich built-in algorithms for atomization metrics, hub detection, path analysis"
        - "Easy serialization to SQLite for game saves"
        - "Well-documented, mature library with active maintenance"
        - "Trivial to test graph logic in isolation"
      negative:
        - "Pure Python slower than C-based alternatives (acceptable at our scale)"
        - "In-memory only: requires explicit save/load logic to persist state"
        - "No concurrent write safety (not needed for single-player turn-based game)"
    performance_notes: |
      NetworkX handles 10,000+ nodes efficiently for in-memory operations.
      Phase 4 projection: 17 entity types × ~100 instances = ~1,700 nodes.
      This is well within NetworkX's comfortable range.
      If future requirements exceed 100,000 nodes, reconsider igraph migration.
    integration_pattern: |
      SQLite (Ledger)         NetworkX (Topology)        ChromaDB (Archive)
        ┌──────────┐           ┌───────────────┐           ┌──────────┐
        │ entities │──startup──▶│ G = nx.DiGraph│◀──query───│ semantic │
        │ edges    │           │ nodes + edges │           │ embeddings│
        └──────────┘           └───────────────┘           └──────────┘
             ▲                        │
             │                        │
             └───────save game────────┘

  ADR011_pure_graph_architecture:
    status: "accepted"
    date: "2024-12-07"
    title: "Pure Graph Architecture: Graph + Math = History"
    context: |
      Following ADR010 (bypass Economy/Politics), a deeper architectural review
      was conducted with multi-AI consensus (Claude + Gemini + User).

      Key insight from Gemini:
      "The previous architecture was trying to simulate INSTITUTIONS.
       The new architecture simulates MATERIAL RELATIONS.
       This is the shift from Liberalism to Materialism."

      The Economy class modeled "the economy" as a liberal abstraction.
      In MLM-TW, "the economy" doesn't exist as a thing - it's the name we give
      to the totality of production relations. The graph makes this explicit.

    decisions:
      delete_legacy_classes:
        action: "Delete Economy and Politics classes immediately"
        rationale: |
          - Not "preserve for future" - that's hoarding technical debt
          - If patterns needed later, write fresh with correct architecture
          - git log is the museum for old code
        files_to_delete:
          - "src/babylon/core/economy.py"
          - "src/babylon/core/politics.py"

      pure_graph_option_c:
        action: "Everything is nodes and edges (Option C)"
        rationale: |
          - Geography is just another relation type
          - Hierarchy exists as CONTAINS edges, doesn't constrain other edges
          - A neighborhood in Global South can have direct edge to hedge fund in NYC
          - Imperialism doesn't respect administrative boundaries - neither should our graph
        edge_types:
          - "CONTAINS (geographic hierarchy)"
          - "EXTRACTS_FROM (economic exploitation)"
          - "REPRESSES (political violence)"
          - "PRODUCES (labor value)"
          - "ORGANIZES (class solidarity)"
        example: |
          USA --[CONTAINS]--> Kansas_City
          Proletariat_KC --[EXTRACTS_FROM]--> Bourgeoisie_NY
          Police_KC --[REPRESSES]--> Proletariat_KC

      hybrid_state_management:
        action: "Snapshots for Engine, Events for Archive"
        rationale: |
          - Engine needs current state to calculate next state (Snapshots)
          - AI narrative needs history of what happened (Events)
          - Math and Story have different data needs - don't confuse them
          - Testing is cleaner: Engine tests are pure functions on snapshots
        structure:
          engine: "Immutable WorldState snapshots"
          archive: "Event log for narrative generation"

      networkx_from_day_one:
        action: "Use NetworkX immediately, not deferred"
        rationale: |
          - If concept is "everything is a graph," implementation should be a graph
          - Using dict for 2 nodes then switching later is chosen technical debt
          - NetworkX is lightweight - no cost for small graphs
          - Built-in algorithms available when needed (centrality, paths)
          - Keeps implementation honest to architecture
        revision: "Supersedes ADR010 recommendation to defer NetworkX"

      simulation_config_is_core:
        action: "SimulationConfig is part of irreducible kernel"
        rationale: |
          - Formulas are parameterized (α, λ, S, k)
          - Without coefficients, formulas are abstract math, not simulation
          - Config is as essential as State and Engine

    the_true_kernel:
      description: "The irreducible core of the simulation"
      components:
        world_state:
          purpose: "The Data"
          contains:
            - "Graph (Nodes: entities)"
            - "Graph (Edges: relations)"
            - "tick: current turn"

        simulation_config:
          purpose: "The Constants"
          contains:
            - "α (extraction_efficiency)"
            - "λ (loss_aversion = 2.25)"
            - "S (subsistence_threshold)"
            - "k (consciousness_sensitivity)"
            - "repression_level"

        formula_library:
          purpose: "The Laws"
          pattern: "f(data, config) → value"
          location: "babylon.systems.formulas"

        simulation_engine:
          purpose: "The Time"
          pattern: "step(state, config) → new_state"

      equation: "Graph + Math = History"

    philosophical_grounding:
      insight: |
        "The Economy is not a class; it is the sum of all EXTRACTS_FROM edges."
        "Politics is not a class; it is the sum of all REPRESSES edges."

        In materialist analysis, abstractions like "the economy" or "politics"
        don't exist as things. They are names for the totality of material relations.
        The graph architecture makes this explicit:
        - No Economy node exists
        - Only relations of extraction, production, exchange
        - "The economy" is a QUERY over those edges, not an entity

      alignment: "Architecture encodes theory. The code IS the analysis."

    consequences:
      positive:
        - "Code matches MLM-TW theoretical framework"
        - "No liberal abstractions hiding material relations"
        - "Flexible: any node can relate to any other node"
        - "Queryable: 'total extraction' is sum of EXTRACTS_FROM edges"
        - "Testable: graph operations are well-understood"
        - "Scalable: NetworkX handles large graphs efficiently"
      negative:
        - "Requires deleting existing code (minimal loss)"
        - "Slightly more abstract than class-based approach"
        - "Team must understand graph thinking"

    consensus:
      participants:
        - "Claude (Opus 4.5)"
        - "Gemini (via user)"
        - "User (Persephone)"
      mantra: "Graph + Math = History"
      supersedes:
        - "ADR010 NetworkX deferral recommendation"

  ADR010_direct_formula_architecture:
    status: "accepted"
    date: "2024-12-07"
    title: "Direct Entities + Formulas architecture (bypass Economy/Politics classes)"
    context: |
      During Phase 2 design review, discovered THREE disconnected math systems:

      1. formulas.py (349 lines, 40 tests)
         - MLM-TW formulas: imperial rent, survival calculus, consciousness drift
         - Pure functions, theoretically grounded, TESTED

      2. Economy class (core/economy.py, 148 lines, 0 tests)
         - Generic economic simulation with its own internal formulas
         - Has update() method but doesn't use formulas.py
         - UNTESTED

      3. Politics class (core/politics.py, 164 lines, 0 tests)
         - Generic political simulation with its own internal formulas
         - Has update() method but doesn't use formulas.py
         - UNTESTED

      4. ContradictionAnalysis (systems/contradiction_analysis.py, 20 tests)
         - Tension tracking and phase transitions
         - Works well but doesn't call formulas.py either

      These systems are completely disconnected. Economy._update_class_relations()
      uses generic formulas, not the MLM-TW formulas we carefully designed and tested.

      Options evaluated:
      A. Replace Economy/Politics entirely with new subsystems
      B. Refactor Economy/Politics to use formulas.py internally
      C. Direct Entities + Formulas (no subsystems)
    decision: |
      Option C: Direct Entities + Formulas architecture for Phase 2.

      SimulationEngine.step() calls formulas directly on entities:

      1. For each Relationship:
         - Call calculate_imperial_rent()
         - Update entity wealth

      2. For each SocialClass:
         - Call calculate_consciousness_drift()
         - Call calculate_acquiescence_probability()
         - Call calculate_revolution_probability()
         - Update entity state

      3. For each Contradiction:
         - Update tension based on entity states
         - Check for rupture/synthesis via ContradictionAnalysis

      No Economy class. No Politics class in the game loop.
      Just: Entities (data) + Formulas (math) + Engine (orchestration)
    rationale:
      - "Simplest possible architecture for 2-node Phase 2"
      - "formulas.py has 40 tests; Economy/Politics have 0 tests"
      - "Avoids two layers of abstraction"
      - "MLM-TW formulas ARE the theory; generic subsystems obscure it"
      - "Single source of truth for calculations"
      - "Easier to trace: entity field -> formula -> new value"
      - "YAGNI: Economy/Politics may be useful later, but not needed now"
    alternatives_rejected:
      replace_economy_politics:
        reason: "Throws away existing code; may be useful for future complexity"
      refactor_to_use_formulas:
        reason: "Adds abstraction layer without benefit for 2-node simulation"
    consequences:
      positive:
        - "Direct traceability from entity to formula to result"
        - "No untested code in critical path"
        - "Simpler debugging: fewer layers"
        - "Engine logic matches theoretical documentation"
      negative:
        - "Economy/Politics classes deleted (see ADR011)"
        - "May need to introduce subsystems when scaling to many entities"
        - "Engine.step() may grow complex (mitigate with clear sections)"
    related_gap_analysis:
      gap_1: "Entity-to-Aggregate: Aggregates computed from entities, not stored"
      gap_2: "Formula-to-Entity wiring: Explicit mapping required (see game-loop-architecture.yaml)"
      gap_3: "This decision resolves the disconnected systems gap"
      gap_4: "Initialization: Factory function for test scenarios"
      gap_5: "Phase transitions: ContradictionAnalysis handles rupture/synthesis"
      gap_6: "Coefficients: SimulationConfig object holds global constants"
      gap_7: "NetworkX: Used from day one per ADR011 (supersedes original ADR010 recommendation)"
      gap_8: "Testing: Property-based tests for feedback loop verification"
    future_considerations: |
      When scaling beyond Phase 2 (many entities, multiple countries),
      may introduce subsystem layer for:
      - Aggregate calculations (GDP computed from all entities)
      - Batch updates (process all entities of a type together)
      - Performance optimization (cache intermediate results)

      Note: Economy/Politics classes were DELETED per ADR011.
      If subsystems are needed in future, write fresh with correct architecture.
      git log preserves the old code for reference if needed.

    implementation_status: |
      IMPLEMENTED in Phase 2 with 704 tests proving:
      - Direct formula wiring works
      - Feedback loops produce emergent behavior
      - No subsystem layer needed for 2-node simulation

  ADR012_service_container:
    status: "accepted"
    date: "2025-12-08"
    title: "ServiceContainer for Dependency Injection"
    context: |
      Phase 2 introduced modular Systems (ImperialRentSystem, ConsciousnessSystem,
      SurvivalSystem, ContradictionSystem). Each System needs access to:
      - SimulationConfig (formula coefficients)
      - FormulaRegistry (hot-swappable formulas)
      - EventBus (publish events like rupture)
      - DatabaseConnection (persistence)

      Passing these as individual parameters would require 4+ parameters per
      System.step() call, making signatures unwieldy and changes difficult.

    decision: |
      Create ServiceContainer dataclass that aggregates all services.
      Systems receive a single ServiceContainer instance instead of
      multiple individual dependencies.

      @dataclass
      class ServiceContainer:
          config: SimulationConfig
          database: DatabaseConnection
          event_bus: EventBus
          formulas: FormulaRegistry

    rationale:
      - "Single parameter simplifies System.step() signatures"
      - "Adding new services doesn't change existing signatures"
      - "Container is frozen/immutable - thread-safe if needed later"
      - "Easy to mock for testing - replace container with test version"
      - "Follows Dependency Injection pattern"

    location: "src/babylon/engine/services.py"

    consequences:
      positive:
        - "Clean System signatures: step(graph, services, context)"
        - "Easy to extend with new services"
        - "Testable via mock containers"
      negative:
        - "One more abstraction layer"
        - "Services accessed via container.config vs directly"

  ADR013_system_protocol:
    status: "accepted"
    date: "2025-12-08"
    title: "System Protocol for Modular Engine Architecture"
    context: |
      ADR010/ADR011 established Direct Entities + Formulas architecture.
      As the engine grew, step() function became monolithic with
      multiple concerns: rent extraction, consciousness drift, survival
      probabilities, tension accumulation.

      Need modular approach where each concern is isolated but can be
      composed into a single game loop.

    decision: |
      Define System protocol with single method:
        step(graph: nx.DiGraph, services: ServiceContainer, context: dict) -> None

      Each System mutates the graph in place following "historical materialist order":
      1. ImperialRentSystem - Economic base (extraction, wealth transfer)
      2. ConsciousnessSystem - Ideological superstructure (consciousness drift)
      3. SurvivalSystem - Political calculation (P(S|A), P(S|R))
      4. ContradictionSystem - Dialectical evolution (tension, rupture)

      SimulationEngine orchestrates Systems via run_tick().

    rationale:
      - "Single Responsibility: each System handles one concern"
      - "Open/Closed: add new Systems without modifying Engine"
      - "Testable: each System can be unit tested in isolation"
      - "Order encodes theory: base before superstructure"
      - "Flexible composition: can run subsets for testing"

    location: "src/babylon/engine/systems/protocol.py"

    systems_implemented:
      - name: "ImperialRentSystem"
        file: "economic.py"
        purpose: "Extract imperial rent from exploitation edges"
      - name: "ConsciousnessSystem"
        file: "ideology.py"
        purpose: "Apply consciousness drift based on material conditions"
      - name: "SurvivalSystem"
        file: "survival.py"
        purpose: "Update P(S|A) and P(S|R) survival probabilities"
      - name: "ContradictionSystem"
        file: "contradiction.py"
        purpose: "Accumulate tension, publish rupture events"

    consequences:
      positive:
        - "Clean separation of concerns"
        - "Easy to add new Systems (e.g., RepresentationSystem for Phase 3)"
        - "Each System independently testable"
        - "Clear execution order documents historical materialist order"
      negative:
        - "Graph mutations must be coordinated (Systems see each other's changes)"
        - "More files to navigate"

    test_count_impact: "Added 17+ tests for System implementations (704 total)"

  ADR014_vertical_slice_scope:
    status: "accepted"
    date: "2025-12-08"
    title: "Vertical Slice Scope for Phase 3 Completion"
    context: |
      Phase 3 aims to add AI narrative generation to the simulation. Before
      horizontal expansion (more entities, locations, phenomena), we need a
      complete vertical slice proving the full data flow works:

      Simulation → Events → RAG Context → LLM Prompt → Narrative Output

      The question is: what is the minimum scope for this vertical slice?

    decision: |
      Vertical slice scope:
      - Entities: Factory worker (proletariat) vs Capitalist (bourgeoisie) - 2 nodes
      - Location: One city (implicit, not modeled as separate entity)
      - Phenomenon: Wealth transfer via Imperial Rent - single economic mechanism
      - LLM: DeepSeek API (OpenAI-compatible, cost-effective)
      - RAG: MVP corpus from Marxists.org (5 curated texts)
      - Verification: Integration tests (not CLI runner)

      Explicitly deferred:
      - Full RAG corpus ingestion (MVP uses 5 texts only)
      - NiceGUI frontend (Phase 4)
      - Multiple scenarios/entity types
      - Save/load game state with narrative
      - Multiple cities or geographic hierarchy
      - Alternative LLM providers (Ollama, Claude)

    rationale:
      - "Proves complete data flow with minimal complexity"
      - "Two-node scenario already proven in Phase 2 integration tests"
      - "Imperial Rent is the core MLM-TW mechanic - wealth extraction"
      - "DeepSeek is cost-effective and OpenAI-compatible (easy integration)"
      - "Marxists.org corpus provides authentic theoretical grounding"
      - "Integration tests enable CI/CD and programmatic verification"
      - "Deferred features can be added after vertical slice validates approach"

    implementation:
      sprint_3_3_deliverables:
        - "LLM Provider protocol + DeepSeek implementation"
        - "Event generation in ImperialRentSystem (exploitation events)"
        - "RAG corpus ingestion script (5 texts from Marxists.org)"
        - "Integration tests proving full narrative pipeline"
      estimated_code: "~400-500 lines"
      estimated_tests: "~20 tests"
      rag_corpus:
        source: "Marxists.org archive"
        texts:
          - "Wage Labour and Capital (Marx, 1847)"
          - "Value, Price and Profit (Marx, 1865)"
          - "Principles of Communism (Engels, 1847)"
          - "Imperialism, the Highest Stage of Capitalism Ch.1-4 (Lenin, 1917)"
          - "The Wretched of the Earth Ch.1 (Fanon, 1961)"
        estimated_chunks: "200-500"

    gaps_addressed:
      gap_1: "No LLM client → DeepSeek Provider implementation"
      gap_2: "No events until rupture → ImperialRentSystem event generation"
      gap_3: "Empty RAG corpus → 5 texts from Marxists.org"
      gap_4: "No verification → Integration tests with assertions"

    consequences:
      positive:
        - "Clear, achievable scope for Sprint 3.3"
        - "Validates architecture before horizontal expansion"
        - "Real theoretical content in RAG (not just static quotes)"
        - "Programmatic testing enables CI/CD"
        - "Deferred features remain possible without rework"
      negative:
        - "Not a 'complete game' - just proves the tech stack"
        - "DeepSeek API requires internet and API key"
        - "Limited corpus may not cover all narrative scenarios"

    success_criteria: |
      Run integration tests, verify:
      1. Simulation advances (tick increases)
      2. Wealth transfers (worker impoverished)
      3. Exploitation events generated and logged
      4. RAG retrieves relevant Marxist theory
      5. DeepSeek API called with proper context
      6. Narrative text generated and captured
      7. All assertions pass in pytest

  ADR015_llm_provider_abstraction:
    status: "accepted"
    date: "2025-12-08"
    title: "LLM Provider Protocol with DeepSeek as Primary"
    context: |
      The NarrativeDirector needs to call an LLM to generate narrative. Multiple
      LLM options exist with different tradeoffs:
      - DeepSeek: Cost-effective, OpenAI-compatible API, good quality
      - Ollama: Local, free, variable quality, no API key needed
      - Claude (Anthropic): Highest quality, higher API costs

      Need abstraction to support all without coupling Director to specific provider.

    decision: |
      Create LLMProvider protocol with DeepSeek as primary implementation:

      @runtime_checkable
      class LLMProvider(Protocol):
          def generate(self, prompt: str, system: str | None = None) -> str:
              '''Generate text from prompt.'''
              ...

      Primary implementation (Sprint 3.3):
      - DeepSeekProvider (OpenAI-compatible API)

      Future implementations (as needed):
      - OllamaProvider (HTTP API)
      - ClaudeProvider (Anthropic SDK)

      NarrativeDirector accepts optional LLMProvider in constructor.
      If None, narrative is logged but not generated (backward compat).

    rationale:
      - "DeepSeek is cost-effective for development/testing"
      - "OpenAI-compatible API means we can use openai Python package"
      - "Protocol pattern matches existing SimulationObserver design"
      - "Optional injection maintains backward compatibility with tests"
      - "Easy to add new providers without modifying Director"
      - "Sync API matches Observer pattern (not async)"

    location: "src/babylon/ai/llm_provider.py"

    deepseek_integration:
      api_base: "https://api.deepseek.com"
      model: "deepseek-chat"
      package: "openai (with custom base_url)"
      env_var: "DEEPSEEK_API_KEY"

    consequences:
      positive:
        - "Cost-effective development and testing"
        - "Flexible provider selection for future"
        - "Easy testing with mock providers"
        - "No vendor lock-in"
        - "OpenAI SDK provides robust client implementation"
      negative:
        - "Requires API key and internet connection"
        - "One more abstraction layer"
        - "Rate limits and API costs (though minimal with DeepSeek)"

  ADR016_fascist_bifurcation:
    status: "accepted"
    date: "2025-12-09"
    title: "The Fascist Bifurcation: Solidarity as Infrastructure"
    context: |
      Sprint 3.4.2 implemented Proletarian Internationalism, but a critical design
      question emerged: should solidarity_strength be auto-calculated from source
      organization, or stored as a persistent edge attribute?

      Historical observation: Accelerationist strategies (crash the economy to
      trigger revolution) consistently produce fascism, not socialism. Germany 1933,
      Italy 1922, Spain 1936. Why?

      Answer: Material disruption (wage decline) creates "agitation energy" that
      has NO INHERENT DIRECTION. It can flow toward class consciousness OR toward
      national chauvinism/scapegoating. The direction depends on PRE-EXISTING
      solidarity infrastructure (unions, internationals, worker organizations).

      If solidarity infrastructure exists: agitation → class awakening → revolution
      If solidarity infrastructure absent: agitation → fascist turn → reaction

      This is the core insight of anti-accelerationism in MLM-TW theory.

    decision: |
      Store solidarity_strength as a PERSISTENT EDGE ATTRIBUTE, not auto-calculated.

      Field: Relationship.solidarity_strength: Coefficient = Field(default=0.0)

      Key semantics:
      - solidarity_strength = 0.0 means NO solidarity infrastructure
      - Must be BUILT through player/system actions (like the 3rd International)
      - High organization without solidarity infrastructure = Fascist Bifurcation risk

      Formula: dΨ_target = σ_edge × (Ψ_source - Ψ_target)
      - σ_edge is READ FROM EDGE DATA, not calculated from source
      - If σ_edge = 0: no transmission even if source is revolutionary
      - This creates the Fascist Bifurcation branch point

    rationale:
      - "Encodes anti-accelerationist theory into mechanics"
      - "Enables emergent Fascist vs Revolutionary outcomes from same starting conditions"
      - "Player agency: must choose to BUILD solidarity before crisis"
      - "Historical accuracy: 3rd International had to be ORGANIZED"
      - "Same code produces both outcomes based on prior state"

    implementation:
      location: "src/babylon/models/entities/relationship.py"
      formula: "src/babylon/systems/formulas.py:calculate_solidarity_transmission()"
      system: "src/babylon/engine/systems/solidarity.py"
      tests: "tests/unit/engine/systems/test_solidarity_system.py"

    scenarios:
      revolutionary_outcome: |
        Setup: P_w (Ψ=0.9), C_w (Ψ=0.1), SOLIDARITY edge (σ=0.8)
        Transmission: delta = 0.8 × (0.9 - 0.1) = 0.64
        Result: C_w awakens, revolutionary consciousness spreads
        Historical analogue: International solidarity during 1917-1919

      fascist_outcome: |
        Setup: P_w (Ψ=0.9), C_w (Ψ=0.1), SOLIDARITY edge (σ=0.0)
        Transmission: delta = 0.0 × (0.9 - 0.1) = 0.0
        Result: C_w loses wages but has no class explanation
        Scapegoating fills the void → Fascist turn
        Historical analogue: Germany 1929-1933

    consequences:
      positive:
        - "Emergent political outcomes from same economic crisis"
        - "Player must invest in solidarity BEFORE crisis"
        - "Encodes 'accelerationism produces fascism' as game mechanic"
        - "Historically accurate: internationals were BUILT, not automatic"
      negative:
        - "More edge state to manage"
        - "Requires UI to show solidarity infrastructure status"

    mantra: "Agitation without solidarity produces fascism, not revolution."

  ADR017_fractal_topology:
    status: "proposed"
    date: "2025-12-09"
    title: "Fractal Topology for Internal Colonies"
    context: |
      The 4-node Imperial Circuit (P_w, P_c, C_b, C_w) models Core-Periphery
      dynamics but treats the Core as monolithic. In reality, the Core contains
      "Internal Colonies" - stratified populations with different relationships
      to imperial rent.

      Examples:
      - Black Americans: super-exploitation within Core, lower access to imperial rent
      - Women workers: gendered wage gap, unpaid reproductive labor
      - Rural workers: geographic stratification, deindustrialized regions
      - Immigrant workers: legal precarity, wage depression

      These groups experience "Periphery-like" conditions within the Core.
      The graph needs to model this without code explosion.

    decision: |
      Use Composite/Graph pattern for fractal topology:
      1. C_w node can be EXPANDED into sub-graph when needed
      2. Sub-nodes (C_w_black, C_w_white, C_w_immigrant, etc.) inherit from parent
      3. Edges can cross fractal levels (C_b → C_w_black)
      4. Aggregation: collapsed C_w = weighted sum of sub-nodes

      Pattern:
      - Phase 3.4: Single C_w node (current)
      - Phase 5: Expand C_w into Internal Colony sub-graph
      - Same engine code works at any fractal level

    rationale:
      - "Models real stratification without monolithic code changes"
      - "Enables 'zoom in' mechanics for UI"
      - "Same formulas apply at every fractal level"
      - "Captures 'internal colonization' of oppressed groups"
      - "Graph-native: NetworkX supports subgraphs naturally"

    implementation_sketch:
      pattern: "Composite Design Pattern + NetworkX subgraph"
      expansion: |
        # Phase 5 implementation
        C_w_expanded = {
          "C_w_white": {..., parent: "C_w", weight: 0.6},
          "C_w_black": {..., parent: "C_w", weight: 0.13},
          "C_w_latino": {..., parent: "C_w", weight: 0.19},
          "C_w_immigrant": {..., parent: "C_w", weight: 0.08},
        }
      aggregation: |
        C_w_aggregate = Σ(sub_node.wealth × sub_node.weight)

    blockers:
      - "Phase 3.4 Multi-Dimensional Consciousness must be complete"
      - "Phase 4 UI needed to visualize fractal zoom"

    consequences:
      positive:
        - "Models Internal Colonies without special-casing"
        - "Same engine handles any fractal depth"
        - "Enables future complexity scaling"
        - "Graph is already the right data structure"
      negative:
        - "Aggregation complexity increases"
        - "UI must handle variable graph depth"
        - "Testing surface grows with fractal levels"

    theoretical_basis:
      sources:
        - "Settlers: The Mythology of the White Proletariat (J. Sakai)"
        - "Black Reconstruction in America (W.E.B. Du Bois)"
        - "Caliban and the Witch (Silvia Federici)"
      insight: |
        The white working class in America has historically been granted
        "psychological wage" (Du Bois) - a sense of superiority that
        compensates for economic exploitation. This is the "internal
        stratification" that fractal topology must model.

  ADR018_logging_strategy:
    status: "accepted"
    date: "2025-12-09"
    title: "Python stdlib logging with custom enhancements"
    context: |
      Babylon needs a logging strategy that supports:
      - Developer debugging (human-readable console)
      - Post-mortem analysis (machine-parseable files)
      - Exception integration (structured error context)
      - Performance awareness (conditional logging for hot paths)
      - Layer-specific concerns (Ledger, Topology, Archive, Observer)

      Options evaluated:
      - Python stdlib logging: Zero deps, universal knowledge, full control
      - loguru: Beautiful output, zero config, but adds dependency
      - structlog: Best structured logging, but overkill for game

      Current state had several issues:
      - logging.yaml references pythonjsonlogger (not in dependencies)
      - __main__.py has basicConfig conflicting with setup_logging()
      - context_window/manager.py logs every add/remove (performance)
      - No log rotation, correlation IDs, or TRACE level

    decision: |
      Stay with Python stdlib logging, add custom enhancements:

      1. Custom JSONFormatter (no dependencies)
         - JSON Lines format for file output
         - ~30 lines of code, no pythonjsonlogger needed

      2. Custom TRACE level (value=5)
         - Ultra-verbose debugging below DEBUG
         - Never enabled in production

      3. Rotating file handlers
         - babylon.log: 10MB x 5 = 50MB max
         - errors.log: ERROR+ only, 5MB x 10 = 50MB

      4. Exception integration
         - BabylonError.log() helper method
         - Log at handling boundary, not raise site
         - Use exception.to_dict() for structured context

      5. Per-simulation logs (future)
         - sim_{timestamp}_{seed}.jsonl for replay/analysis

    rationale:
      - "User preference: minimal dependencies"
      - "User preference: loves custom loggers"
      - "Exception hierarchy already has to_dict() - leverage it"
      - "stdlib is universal - any Python dev can work with it"
      - "Self-contained project - no library interop concerns"
      - "Custom enhancements are simple (~100 lines total)"

    log_levels:
      CRITICAL: "System unusable, data loss imminent"
      ERROR: "Operation failed, system continues"
      WARNING: "Unexpected but handled"
      INFO: "Normal operation milestones"
      DEBUG: "Detailed diagnostics"
      TRACE: "Ultra-verbose (custom, value=5)"

    output_formats:
      console:
        format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
        level: "INFO (default)"
      file:
        format: "JSON Lines"
        level: "DEBUG"
        rotation: "10MB x 5 backups"

    exception_logging_principle: |
      "Log at the handling boundary"
      - Don't log at raise site (causes duplicates on re-raise)
      - Log at final handler (where exception is resolved)
      - Use exception.to_dict() for structured context
      - Caller decides importance (retry? fallback? fatal?)

    implementation:
      files_to_create:
        - "src/babylon/utils/log.py (JSONFormatter, TRACE, helpers)"
      files_to_modify:
        - "src/babylon/config/logging_config.py (rotation, handlers)"
        - "src/babylon/__main__.py (remove basicConfig)"
        - "src/babylon/utils/exceptions.py (add log() method)"
        - "logging.yaml (fix pythonjsonlogger reference)"
        - "src/babylon/rag/context_window/manager.py (add guards)"

    detailed_spec: "ai-docs/logging-architecture.yaml"

    consequences:
      positive:
        - "Zero new dependencies"
        - "Custom formatters exactly fit our needs"
        - "Exception error_code in all log entries"
        - "Machine-parseable JSONL for analysis"
        - "Rotation prevents disk bloat"
        - "TRACE available for deep debugging"
      negative:
        - "Must maintain custom code (~100 lines)"
        - "No fancy log aggregation features out of box"
        - "Correlation ID propagation is manual"

    alternatives_rejected:
      loguru:
        reason: "Adds dependency; different API requires learning curve"
      structlog:
        reason: "Overkill complexity for game project; steep learning curve"
      pythonjsonlogger:
        reason: "Can write equivalent in 30 lines; avoids dependency"

  ADR019_exception_hierarchy:
    status: "accepted"
    date: "2025-12-09"
    title: "Unified exception hierarchy (33 → 10 classes)"
    context: |
      Original codebase had 33 exception classes across multiple modules with:
      - Unclear inheritance relationships
      - Inconsistent error code usage
      - No structured serialization
      - Proliferating special-case exceptions

      Needed: Clear hierarchy aligned with architectural layers (Embedded Trinity)
      that supports machine-parseable error codes and structured logging.

    decision: |
      Consolidate to 10 core exception classes using Mikado Method refactoring:

      BabylonError (base)
      ├── InfrastructureError (retryable I/O)
      │   ├── DatabaseError
      │   └── StorageError → CheckpointIOError → 3 children
      ├── ValidationError (bad input)
      │   └── ConfigurationError
      ├── SimulationError (fatal engine)
      │   └── TopologyError
      └── ObserverError (non-fatal AI/RAG)
          ├── LLMError
          └── RagError → 15 aliases for backwards compatibility

      All exceptions have:
      - message: Human-readable description
      - error_code: PREFIX_NNN format (e.g., LLM_002, RAG_401)
      - details: dict for structured context
      - to_dict(): JSON-serializable representation

    rationale:
      - "Layer alignment: Exceptions match Embedded Trinity architecture"
      - "Semantic codes: PREFIX_NNN enables machine parsing and filtering"
      - "Catch hierarchy: 'except ObserverError' catches all AI/RAG errors"
      - "Non-fatal observers: AI failures never crash simulation (ADR003)"
      - "Backwards compatibility: 15 aliases preserve old exception names"
      - "Structured logging: to_dict() integrates with JSONFormatter"

    error_code_prefixes:
      SYS: "System-level / base errors"
      INFRA: "Infrastructure errors"
      STOR: "Storage/file errors"
      DB: "Database errors"
      VAL: "Validation errors"
      CFG: "Configuration errors"
      SIM: "Simulation errors"
      TOP: "Topology/graph errors"
      OBS: "Observer layer errors"
      LLM: "LLM generation errors"
      RAG: "RAG system errors (100-599 ranges)"

    key_principle: |
      Log at the HANDLING boundary, not at the raise site.
      The caller knows if an exception is worth logging.
      Use exception.to_dict() for structured context.

    test_coverage:
      file: "tests/unit/utils/test_exceptions.py"
      count: 93
      categories:
        - "Hierarchy inheritance"
        - "Default error codes"
        - "Custom error codes"
        - "Constructor behavior"
        - "Serialization (to_dict, __str__, __repr__)"
        - "Backwards compatibility aliases"
        - "Exception propagation"
        - "Import path consistency"
        - "Edge cases (pickle, unicode, chaining)"

    files_modified:
      - "src/babylon/utils/exceptions.py (core hierarchy)"
      - "src/babylon/exceptions.py (re-exports)"
      - "src/babylon/rag/exceptions.py (RagError + aliases)"
      - "src/babylon/rag/context_window/__init__.py (aliases)"
      - "src/babylon/engine/history/io.py (checkpoint exceptions)"

    consequences:
      positive:
        - "Clear 4-layer hierarchy matches architecture"
        - "Machine-parseable error codes for log analysis"
        - "Structured to_dict() for JSON logging"
        - "Single catch point per layer (e.g., ObserverError)"
        - "93 tests lock down behavior"
        - "Backwards compatible via aliases"
      negative:
        - "Must remember to use appropriate error_code"
        - "Aliases may confuse new developers"
        - "Some module-specific exceptions still exist (CheckpointIOError)"

    detailed_spec: "ai-docs/exceptions-architecture.yaml"

  ADR020_parameter_tuning_methodology:
    status: "accepted"
    date: "2025-12-11"
    title: "Sensitivity Analysis for GameDefines Tuning"
    context: |
      GameDefines contains all simulation coefficients externalized via the Paradox Refactor.
      However, initial values were educated guesses, not empirically validated.

      Problem discovered: extraction_efficiency = 0.8 (default) kills the periphery in
      ~9 ticks, making the game unplayable. No dynamics have time to develop.

      Need systematic approach to find "Playable Boundaries" - parameter values where
      the simulation is challenging but allows meaningful gameplay.

    decision: |
      Create tools/tune_parameters.py for automated sensitivity analysis:

      1. Parameter Sweep: Iterate through value ranges (start, end, step)
      2. Simulation Run: Execute 50 ticks per parameter value
      3. Death Detection: Track when periphery wealth <= 0.001
      4. Metrics Collection: ticks_survived, max_tension, outcome
      5. Boundary Analysis: Find value where survival crosses threshold

      Mise tasks:
      - mise run tune-params (default extraction_efficiency sweep)
      - mise run tune-params-custom (custom parameter sweeps)

      Findings recorded in ai-docs/balance-tuning.yaml.

    rationale:
      - "Empirical validation beats theoretical guessing"
      - "Reproducible: same sweep produces same results (deterministic sim)"
      - "Automated: can re-run after any formula changes"
      - "Documented: balance-tuning.yaml captures findings for future reference"
      - "Reveals design issues: Tension Inversion anomaly discovered this way"

    tool_location: "tools/tune_parameters.py"
    findings_location: "ai-docs/balance-tuning.yaml"
    mise_tasks:
      - "tune-params"
      - "tune-params-custom"

    key_findings:
      extraction_efficiency:
        default: 0.8
        playable_boundary: 0.25
        recommended: 0.20
        issue: "Default kills periphery in 9 ticks - unplayable"

      tension_inversion_anomaly:
        observation: "Higher extraction → LOWER max tension"
        cause: "Death occurs before tension accumulates"
        implication: "Current model may need tension rate adjustment"

    consequences:
      positive:
        - "Data-driven balance decisions"
        - "Reproducible methodology"
        - "Catches unplayable configurations before release"
        - "Documents rationale for parameter values"
        - "Reveals formula design issues early"
      negative:
        - "Each sweep takes ~30 seconds (50 ticks × 10 values)"
        - "Only tests one parameter at a time (no interaction effects)"
        - "Requires manual analysis of anomalies"

    future_enhancements:
      - "Multi-parameter grid search"
      - "Automated anomaly detection"
      - "Visualization of parameter landscapes"

  ADR026_nicegui_root_function_pattern:
    status: "accepted"
    date: "2025-12-14"
    title: "NiceGUI Root Function Pattern (No @ui.page decorator)"
    context: |
      NiceGUI has two mutually exclusive modes that cannot be mixed:

      1. **Script Mode**: UI elements in global scope, no decorators
         - ui.label("Hello")
         - ui.run()

      2. **Page Mode**: @ui.page decorators OR ui.run(root_func)
         - @ui.page("/")
         - def index(): ui.label("Hello")
         - ui.run()

      Initial implementation mixed @ui.page("/") decorator with ui.timer() at
      global scope, causing RuntimeError at startup:
      "ui.page cannot be used in NiceGUI scripts when UI is defined in the global scope."

    decision: |
      Use Root Function Pattern for single-page applications:

      def main_page() -> None:
          ui.dark_mode().enable()
          # All UI elements inside root function
          ui.timer(interval=1.0, callback=run_loop)  # Timer inside!

      ui.run(main_page, title="...", port=6969)

      Key rules:
      1. NO @ui.page decorator for single-page apps
      2. ALL UI elements inside the root function
      3. ui.timer() must be inside root function, not global scope
      4. Pass root function to ui.run() as first positional argument

    rationale:
      - "Single-page MVP doesn't need multi-page routing"
      - "Root function pattern is cleaner and more testable"
      - "Prevents mode conflict RuntimeError"
      - "NiceGUI docs recommend this for simple applications"
      - "Timer inside root ensures proper page context"

    implementation:
      main_file: "src/babylon/ui/main.py"
      tests: "tests/unit/ui/test_main.py"
      test_approach: "AST-based detection of @ui.page decorator and ui.run() pattern"

    consequences:
      positive:
        - "No RuntimeError at startup"
        - "Clear separation: root function owns all UI"
        - "Testable: can verify patterns via AST parsing"
        - "Simple: no routing complexity for MVP"
      negative:
        - "Cannot use @ui.page for additional routes without refactoring"
        - "Must remember timer goes INSIDE root function"

    related_docs:
      - "docs/how-to/gui-development.rst"
      - "brainstorm/ui/digital-grow-room.md"

  ADR027_frontend_stack:
    status: "accepted"
    date: "2025-12-14"
    title: "NiceGUI (Native Mode) with Tailwind CSS"
    context: |
      Requirement for "Bunker Constructivism" aesthetic and async simulation loop.
      Target is Steam executable via pywebview wrapper.

      Options evaluated:
      - NiceGUI (web mode): Browser-based, good for development
      - NiceGUI (native mode): pywebview wrapper, standalone .exe
      - Tkinter: Native Python GUI, limited styling
      - PyQt/PySide: Heavy dependency, complex licensing

    decision: |
      Use NiceGUI in Native Mode (pywebview) with Tailwind CSS styling.
      Deprecate all Tkinter/pack patterns from documentation.

      Key patterns:
      - Root function mode (per ADR026)
      - Tailwind utility classes for all styling
      - Component class strings defined in ai-docs/design-system.yaml
      - CRT overlay effect via CSS pseudo-elements

    rationale:
      - "Native async support for EventBus integration"
      - "Tailwind enables 'CSS painting' for specific Bunker aesthetic"
      - "Native mode provides .exe wrapper for Steam distribution"
      - "Root function pattern already validated (ADR026)"
      - "Single codebase for dev (web) and release (native)"

    deprecates:
      - "All Tkinter references in docs/how-to/gui-development.rst"
      - "pack/grid geometry manager patterns"
      - "Soviet Red color scheme (replaced by Bunker Constructivism)"

    implementation:
      design_system: "ai-docs/design-system.yaml"
      ui_code: "src/babylon/ui/"
      aesthetic_spec: "brainstorm/ui/digital-grow-room.md"

    consequences:
      positive:
        - "Modern CSS capabilities via Tailwind"
        - "Consistent styling via design-system.yaml"
        - "Async-native matches simulation EventBus"
        - "Easy Steam distribution via pywebview"
      negative:
        - "Requires web runtime (Chromium via pywebview)"
        - "Larger binary than pure Tkinter"
        - "Learning curve for Tailwind utility classes"

  ADR028_code_complexity_gating:
    status: "accepted"
    date: "2025-12-26"
    title: "Cyclomatic Complexity Gating for AI-Heavy Development"
    context: |
      Babylon is developed with ~80% AI-written code. This creates a unique quality
      challenge: AI can produce code that passes tests but has sprawling complexity
      that becomes a debugging nightmare.

      Earlier in Phase 3, we encountered code smells and maintainability issues that
      prompted investigation into static analysis tooling. The question arose:
      "What guardrails prevent AI from generating unmaintainable code?"

      Key constraints:
      - Must not block local development velocity
      - Must catch problems before main branch
      - Should integrate with existing tooling (Ruff, pre-commit, Mise, GitHub Actions)
      - Thresholds must be pragmatic for existing codebase

      Options evaluated:
      - Radon: Full complexity metrics, detailed reporting
      - Xenon: Radon wrapper with CI-friendly thresholds
      - Ruff C901: McCabe complexity rule, fast pre-commit integration
      - SonarQube: Heavy, enterprise-focused (overkill)
      - wily: Trend tracking over time (future enhancement)

    decision: |
      Implement three-layer complexity checking:

      1. **Pre-commit (Ruff C901)**: Fast local feedback
         - Rule: max-complexity = 15
         - Blocks commit if any function exceeds CC 15
         - Immediate developer feedback

      2. **CI Report (Radon)**: Visibility on all branches
         - Command: poetry run radon cc src/ -a -s
         - Shows per-function grades in CI logs
         - Never blocks, informational only

      3. **CI Gate (Xenon)**: Blocking on main only
         - Command: poetry run xenon --max-absolute C src/
         - Fails merge if any function exceeds CC 15 (grade C boundary)
         - Quality gate before stable releases

      Thresholds:
      - Warning: CC > 10 (shown in reports)
      - Blocking: CC > 15 (fails CI on main)

      Scope:
      - Included: src/babylon/**/*.py
      - Excluded: tests/**/*.py (test functions intentionally verbose)

      Metrics selected:
      - Cyclomatic Complexity: YES (primary, actionable)
      - Maintainability Index: NO (docstrings inflate it artificially)
      - Halstead metrics: NO (too academic, high noise)
      - Raw LOC: NO (already have 100-line rule in CLAUDE.md)

    rationale:
      - "80% AI-written code needs guardrails humans wouldn't require"
      - "CC > 15 is stricter than some enterprises but pragmatic for existing codebase"
      - "Google uses 10, Microsoft uses 15 - we're in the reasonable middle"
      - "Three layers give fast feedback + visibility + final gate"
      - "Ruff C901 in pre-commit catches issues before they leave developer machine"
      - "Radon reports enable 'seeing the stats' without blocking velocity"
      - "Xenon on main ensures releases meet quality bar"
      - "Excluding tests prevents false positives from verbose test setups"
      - "Can tighten to CC 10 later after baselining current state"

    alternatives_rejected:
      maintainability_index:
        reason: "Mandatory Sphinx docstrings inflate MI artificially"
      sonarqube:
        reason: "Enterprise-focused, requires server, overkill for game project"
      blocking_on_dev:
        reason: "Would slow velocity during sprint work; warnings sufficient for integration branch"
      cc_threshold_10:
        reason: "Too strict for existing codebase; start at 15, tighten later"

    implementation:
      packages:
        - "radon (poetry add --group dev radon)"
        - "xenon (poetry add --group dev xenon)"
      config_changes:
        - "pyproject.toml: [tool.ruff.lint] extend-select = ['C901'], max-complexity = 15"
        - ".github/workflows/ci.yml: Add radon report step (all branches)"
        - ".github/workflows/ci.yml: Add xenon gate step (main only)"
      mise_tasks:
        - "complexity: Show full radon report"
        - "complexity-check: Run xenon gate locally"
      pre_commit:
        - "Ruff C901 rule enabled via existing ruff hook"

    workflow_diagram: |
      Developer writes code
              │
              ▼
      Pre-commit (Ruff C901) ──► Fails locally if CC > 15
              │
              ▼
      Push to feature/*
              │
              ▼
      PR to dev ──► Radon report (visible, never blocks)
              │
              ▼
      Merge to dev ──► Same (warnings only)
              │
              ▼
      PR to main ──► Xenon gate (BLOCKS if CC > 15)
              │
              ▼
      Merge to main (clean)

    industry_context:
      google: "CC ≤ 10 recommended"
      microsoft: "CC ≤ 15 for maintainable code"
      nasa_power_of_10: "Implies CC around 10"
      sonarqube_default: "10 (cognitive complexity variant)"
      babylon_choice: "15 (pragmatic, can tighten later)"

    consequences:
      positive:
        - "AI cannot merge sprawling functions to main"
        - "Immediate feedback via pre-commit catches issues early"
        - "CI visibility enables informed discussions in PRs"
        - "Pragmatic threshold prevents false positive fatigue"
        - "Future tightening possible without architectural change"
        - "Aligns with existing 100-line function rule"
      negative:
        - "Two new dev dependencies (radon, xenon)"
        - "Slight CI time increase (~2-3 seconds)"
        - "Must baseline existing code before enabling gate"
        - "Some legitimate high-CC code may need exclusions"

    detailed_spec: "ai-docs/tooling.yaml (code_complexity section)"

    related:
      - "ADR021_cicd_philosophy"
      - "CLAUDE.md 100-line function rule"
      - "ai-docs/anti-patterns.yaml"

    mantra: "When AI writes the code, machines must also guard the quality."

  ADR029_hybrid_graph_architecture:
    status: "accepted"
    date: "2025-12-26"
    title: "Hybrid Graph Architecture: NetworkX (Tactical) + KuzuDB (Strategic)"
    context: |
      Epoch 2 ("The War") requires continental-scale simulation with the full US
      administrative hierarchy (~74,000 Territory nodes) and complex historical
      queries ("The Chronicle") that exceed RAM limits of pure NetworkX.

      Key requirements driving this decision:
      1. **Scale**: 74k nodes = ~50 States + ~3,000 Counties + ~70,000 Cities/Townships
      2. **Dual Topology**: Must model "Land as Material" (physical mesh) AND
         "State as Hierarchy" (administrative DAG) simultaneously
      3. **Historical Queries**: "The Chronicle" needs to answer "what was California's
         state in tick 50?" - requires efficient disk-backed storage with time travel
      4. **Front Line Queries**: Finding territories adjacent to enemy-controlled land
         is O(N) iteration with NetworkX but O(1) with graph query language

      Epoch 1 ("The Skirmish") works fine with pure NetworkX + SQLite because:
      - Small scale (4-node Imperial Circuit)
      - No historical queries needed
      - All state fits in RAM

      The question: How do we scale to Epoch 2 without abandoning the working Epoch 1?

    decision: |
      Adopt **KuzuDB** (embedded columnar graph database) for Epoch 2 persistence
      while keeping NetworkX for active simulation. This creates a two-tier architecture:

      **Tier 1: KuzuDB ("Strategic Map")**
      - Stores the complete World Graph (all ~74k Territory nodes)
      - Stores historical snapshots ("The Chronicle")
      - Handles complex graph queries in Cypher-like syntax
      - Disk-backed columnar storage (infinite scale potential)
      - File: data/epoch2_world.kuzu (embedded, no server)

      **Tier 2: NetworkX ("Tactical Cache")**
      - Holds active simulation "Sector" (subset of nodes being simulated)
      - Fast in-memory operations for tick calculations
      - Same engine code as Epoch 1 (no rewrite needed)
      - Hydrated from KuzuDB at sector boundaries

      **Dual-Graph Schema in KuzuDB:**
      1. `:ADMINISTERS` edges (DAG): Federal → State → County → City
         - Represents legal/administrative hierarchy
         - Properties: control_level, legal_status
      2. `:ADJACENT_TO` edges (Mesh): Physical adjacency between territories
         - Represents geographic reality (borders)
         - Properties: barrier_type, permeability

      **Hydration Protocol:**
      - Engine loads "Sector" (N-hop neighborhood around conflict zone) into NetworkX
      - Simulation runs on NetworkX cache (existing Systems unchanged)
      - At tick boundary, flush changes back to KuzuDB
      - Write historical snapshot to Chronicle tables

      Epoch 1 code remains **100% unchanged**. This is strictly additive infrastructure.

    rationale:
      embedded_trinity_preserved:
        principle: "No external servers"
        solution: "KuzuDB is embedded like SQLite - single file, no daemon"

      front_line_query_problem:
        problem: "Find all territories adjacent to enemy-controlled land"
        networkx_solution: "O(N) iteration over all edges"
        kuzu_solution: |
          MATCH (friendly:Territory)-[:ADJACENT_TO]-(hostile:Territory)
          WHERE friendly.controller = 'PLAYER' AND hostile.controller = 'STATE'
          RETURN friendly
        improvement: "O(1) graph traversal vs O(N) iteration"

      land_is_material:
        principle: "Territory is material base, not abstract container"
        solution: "Disk-backed columnar storage allows massive scale without RAM pressure"

      the_chronicle:
        requirement: "Query historical states for narrative and analysis"
        solution: "Append-only history tables in KuzuDB enable time-travel queries"

      incremental_adoption:
        principle: "Don't break what works"
        solution: "Epoch 1 untouched; Epoch 2 adds new persistence layer"

    implementation:
      new_files:
        - "ai-docs/epoch2-persistence.yaml (specification)"
        - "tools/init_epoch2_db.py (schema initialization)"
        - "src/babylon/data/kuzu_manager.py (future: hydration logic)"

      new_dependency:
        package: "kuzu"
        version: ">=0.4.0"
        install: "poetry add kuzu"
        rationale: "Embedded graph DB, pip-installable, active development"

      node_schema:
        Territory:
          id: "str (e.g., 'US-CA-037' for LA County)"
          name: "str"
          type: "enum (OGV, OPC)"
          area_sq_km: "float"
          bioregion: "str"
          habitability: "float [0-1]"
          heat: "float [0-1]"

      edge_schemas:
        ADMINISTERS:
          from: "Territory"
          to: "Territory"
          properties:
            control_level: "float [0-1]"
            legal_status: "enum (DE_JURE, DE_FACTO, CONTESTED, SUBVERSIVE)"

        ADJACENT_TO:
          from: "Territory"
          to: "Territory"
          properties:
            barrier_type: "enum (NONE, RIVER, MOUNTAIN, HIGHWAY, BORDER)"
            permeability: "float [0-1]"

    consequences:
      positive:
        - "Infinite scale potential - 74k nodes trivial for KuzuDB"
        - "Enables 'The Chronicle' historical query engine"
        - "Front Line queries become O(1) graph traversal"
        - "Embedded Trinity preserved (no external servers)"
        - "Epoch 1 code 100% unchanged"
        - "Dual-graph schema models both hierarchy AND adjacency"
        - "Cypher-like query language is expressive and readable"

      negative:
        - "New dependency (kuzu package)"
        - "Sync complexity between Tactical Cache and Strategic Map"
        - "Learning curve for KuzuDB/Cypher syntax"
        - "Potential consistency issues if flush fails mid-tick"

    mitigation:
      sync_complexity: "Create HydrationManager class encapsulating all Kuzu interaction"
      consistency: "Use KuzuDB transactions for atomic flush operations"
      testing: "Pytest fixtures for in-memory Kuzu instance"

    related_decisions:
      - "ADR001_embedded_trinity (architecture principle)"
      - "ADR009_networkx_topology (Epoch 1 graph choice)"
      - "ADR002_sqlite_over_postgres (embedded database preference)"

    supersedes: null  # This extends, not replaces, existing architecture

    mantra: "Strategic Map stores the world. Tactical Cache runs the war."

    # =========================================================================
    # AMENDMENT: Dynamic Sovereignty Refactor (2025-12-26)
    # =========================================================================

    amendments:
      - id: "ADR029-A1"
        date: "2025-12-26"
        title: "Dynamic Sovereignty Refactor"
        status: "accepted"

        context: |
          The original ADR029 schema represented territory control via a `controller`
          property on Territory nodes (e.g., controller="STATE"). This is insufficient
          for Epoch 3 requirements:
          - Civil wars (multiple competing sovereignty claims)
          - Secession/Balkanization (new nations emerging)
          - Dual power situations (revolutionary control without legal recognition)

          Property-based sovereignty requires O(N) updates to change borders and
          cannot model contested territories or multiple claimants.

        decision: |
          **Sovereignty is a Node, not a Property.**

          Refactor the schema to treat sovereignty as first-class graph entities:

          1. **New Node Type: `Sovereign`**
             - Represents political entities capable of claiming territory
             - Properties: id, name, sovereignty_type, legitimacy, color_hex
             - Examples: "United States Federal Government", "Provisional Revolutionary Command"

          2. **New Edge Type: `CLAIMS` (Sovereign → Territory)**
             - Represents sovereignty assertions over territory
             - Properties: control_level (0-1), fiscal_status, legal_status
             - Multiple CLAIMS edges can exist for contested territories

          3. **Removed: `controller` property from Territory**
             - Sovereignty now determined by incoming CLAIMS edges
             - Effective controller = Sovereign with highest control_level

          **The Fracture Operation:**
          Sovereignty transfers are now O(1) edge operations:
          ```cypher
          // Transfer LA from USA to Revolutionary Command
          MATCH (usa:Sovereign {id: 'SOV_USA_FED'})-[c:CLAIMS]->(la:Territory)
          SET c.control_level = 0.0

          MATCH (prc:Sovereign {id: 'SOV_PRC'})-[c:CLAIMS]->(la:Territory)
          SET c.control_level = 1.0
          ```

        rationale:
          o1_border_changes:
            problem: "Secession requires updating controller property on many territories"
            solution: "Edge rewiring is O(1) - delete old CLAIMS, create new CLAIMS"

          civil_war_modeling:
            problem: "Cannot represent multiple competing sovereignty claims"
            solution: "Multiple CLAIMS edges with different control_levels"

          dual_power:
            problem: "Cannot distinguish legal authority from actual control"
            solution: "CLAIMS edge has both legal_status and control_level properties"

          layer_independence:
            principle: "Land (geography) and State (politics) are independent"
            solution: "ADJACENT_TO edges (Land) are unaffected by CLAIMS changes (State)"

        consequences:
          positive:
            - "O(1) border changes via edge rewiring"
            - "Civil war modeling with competing sovereignty claims"
            - "Dual power situations (de jure vs de facto control)"
            - "New nations can emerge dynamically (create Sovereign node)"
            - "Legitimacy tracking per sovereign entity"
            - "Land/State layer separation preserved"

          negative:
            - "More complex queries (must traverse CLAIMS edges)"
            - "Schema migration required from v1.0.0"
            - "Additional validation needed (control_level sum constraints)"

        implementation:
          spec_file: "ai-docs/epoch2-persistence.yaml (v1.1.0)"
          init_script: "tools/init_epoch2_db.py (updated)"

          new_schemas:
            - "Sovereign node table"
            - "CLAIMS edge table (Sovereign → Territory)"
            - "ClaimsSnapshot table (Chronicle)"

          removed:
            - "Territory.controller property"
            - "controller index on Territory"

        related:
          - "ai-docs/epoch2-persistence.yaml"
          - "tools/init_epoch2_db.py"

# =============================================================================
# PENDING DECISIONS
# =============================================================================

pending:

  PDR001_ui_framework:
    status: "pending"
    options:
      - "NiceGUI (Python-native)"
      - "Obsidian-like custom renderer"
      - "Terminal-only (Rich)"
    leaning: "NiceGUI for game, Obsidian-like for wiki"
    blockers: "Game loop needs to exist first"
    aesthetic_direction:
      name: "The Digital Grow Room"
      status: "approved"
      spec: "brainstorm/ui/digital-grow-room.md"
      summary: |
        Hydroponic Cyber-Insurgency aesthetic. You are the virus in the machine,
        the internal contradiction growing revolution in the basement of empire.
        Jury-rigged high-tech: GPUs cooling in a damp basement, purple LED grow
        lights reflecting off CRT monitors, terminal green data streams.
      palette:
        void: "#050505 (dark basement)"
        grow_light: "#9D00FF (life of revolution)"
        data: "#39FF14 (terminal green)"
        heat: "#FF3333 (rupture/danger)"
      typography:
        - "JetBrains Mono (code/data)"
        - "VCR OSD Mono (glitch headers)"
      metaphors:
        topology: "Attack Surface / Network Scanner"
        narrative: "Root Shell / Terminal stdout"
        controls: "Environmental Controls (fan speed, voltage)"
        tension: "GPU Temperature gauge"
        repression: "Heat Signature warning"
        resources: "Power Draw meter"
        ai: "The Gardener"
      widgets:
        doomsday_clock:
          system: "Time"
          concept: "Clock showing tick count, hand approaches midnight"
          visual: "Bulletin of Atomic Scientists aesthetic, glows red near 12:00"
          thematic: "The revolution is inevitable. The only question is when."
        sankey_diagram:
          system: "Imperial Rent"
          concept: "Wealth flow visualization Periphery → Core"
          visual: "Thick flowing lines, gradient color, animated particles"
          thematic: "Make the invisible visible. Show where wealth actually goes."
        tension_gauge:
          system: "Contradiction"
          concept: "Analog needle gauge, glass cracks at 1.0"
          visual: "Retro industrial gauge, green→yellow→red zones"
          thematic: "Pressure builds. The system shatters."
        political_compass:
          system: "Consciousness"
          concept: "Scatter plot of population ideological drift"
          visual: "X=Class Consciousness, Y=Internationalism, dots drift over time"
          thematic: "Watch the masses awaken. Or fall to fascism."
        dossier_modal:
          system: "Inspection"
          concept: "Click node → detailed Ledger data modal"
          visual: "Tabbed interface: Overview | Raw JSON | History | Relationships"
          thematic: "Intelligence gathering. Know your enemy. Know your comrades."

  PDR002_narrative_delivery:
    status: "brainstorm"
    title: "The Gramscian Wire - Narrative Delivery System"
    reference: "brainstorm/narrative/gramscian-wire.md"
    supersedes: "brainstorm/gramscian-wiki-engine.md (original wiki concept)"
    pivot: |
      From "standalone wiki engine" to "Victoria-style newspaper announcements"
      with Gramscian hegemony mechanics. The wiki IS the narrative delivery.
      You're not reading newspapers - you're intercepting information streams.
    concept:
      name: "The Wire"
      channels:
        corporate_feed: "Hegemonic perspective, 'neutral' bourgeois framing"
        liberated_signal: "Counter-hegemonic, revolutionary perspective"
        intel_briefing: "Player faction analysis, actionable data"
      key_insight: "Same event, different framings - ideology made visible"
    new_mechanics:
      hegemony_stat:
        description: "Faction attribute for cultural/narrative control"
        effect: "Determines which perspective is 'mainstream'"
      propaganda_actions:
        - "Underground press"
        - "Radio broadcast"
        - "Viral campaign"
      reader_effect: "What player reads affects consciousness (optional)"
    implementation_phases:
      phase_1: "Single perspective (current NarrativeDirector)"
      phase_2: "Dual perspective (Bourgeois + Revolutionary)"
      phase_3: "Full N-faction with hegemony mechanics"
    theoretical_basis: |
      Gramsci's cultural hegemony: Ruling class maintains power through
      consent, not just coercion. Media naturalizes capitalist worldview.
      Counter-hegemony provides alternative framing.
    integration:
      extends: "src/babylon/ai/director.py (NarrativeDirector)"
      ui: "brainstorm/ui/digital-grow-room.md (The Wire panel)"

  ADR021_cicd_philosophy:
    status: "accepted"
    date: "2025-12-11"
    title: "CI/CD Philosophy: Block on Correctness, Not Style"
    context: |
      Initial CI pipeline had 6+ jobs including yamllint, markdownlint, and
      Python 3.13 matrix testing. This created friction:
      - Prose in ai-docs/ failed yamllint line length checks
      - Style differences blocked merges when pre-commit wasn't run locally
      - Python 3.13 tests were informational noise (not production target)

      Question: What should CI actually block on for a game project?

    decision: |
      Simplified CI to 3 jobs with clear philosophy:

      1. **ci** (blocks merge): Lint, Type Check, Test
         - Ruff check (likely bugs)
         - MyPy strict (type errors)
         - Pytest -m "not ai" (functionality)

      2. **docs** (blocks merge): Documentation Build
         - Sphinx -W (warnings as errors)
         - Doctest modules (example code works)

      3. **style** (informational): Style Check
         - Ruff format --check
         - continue-on-error: true (never blocks)

      Extended analysis (releases/weekly) in separate workflow:
      - Python version matrix (3.12, 3.13)
      - Parameter sweep analysis
      - AI/RAG evaluation tests

    rationale:
      - "This is a game, not enterprise banking software"
      - "Style is local concern (pre-commit hooks)"
      - "CI should catch bugs, not formatting"
      - "Reduce friction = more commits = faster iteration"
      - "Extended tests run when useful, not on every PR"

    files:
      - ".github/workflows/ci.yml"
      - ".github/workflows/extended-analysis.yml"

    consequences:
      positive:
        - "Faster CI runs (~2 min vs ~5 min)"
        - "Less false-positive blocking"
        - "Clear separation of concerns"
        - "ai-docs/ can have prose-length lines"
      negative:
        - "Style issues may slip through if pre-commit skipped"
        - "Python 3.13 compatibility not continuously tested"

    related_mantras:
      - "block_on_correctness"
      - "test_logic_not_ai"

  ADR022_sphinx_autosummary_config:
    status: "accepted"
    date: "2025-12-11"
    title: "Configure Sphinx to Handle Python Re-Exports"
    context: |
      Sphinx documentation build produced 2016 warnings, primarily:
      "duplicate object description of babylon.engine.Event"

      Investigation revealed the root cause:

      Python's __init__.py re-exports classes for convenient imports:
      ```python
      # babylon/engine/__init__.py
      from babylon.engine.event_bus import Event
      __all__ = ["Event", ...]
      ```

      This is STANDARD Python API design (used by pandas, requests, pydantic).
      Users import `from babylon.engine import Event` instead of the full path.

      Sphinx's autosummary with `autosummary_generate = True` documents
      classes in BOTH locations:
      1. babylon.engine.Event (from re-export)
      2. babylon.engine.event_bus.Event (from original)

      This creates duplicate cross-reference warnings.

    decision: |
      Configure Sphinx, don't restructure Python code.

      Key configuration in docs/conf.py:
      ```python
      # Only document members where they're DEFINED, not IMPORTED
      autosummary_imported_members = False
      ```

      This tells Sphinx: when Event is imported into __init__.py,
      don't document it again there. Document it only in event_bus.py.

    rationale:
      - "Re-exports are intentional API design, not a mistake"
      - "Same object in memory: Event1 is Event2 → True"
      - "Users expect `from babylon.engine import Event`"
      - "Flattening __init__.py would break imports across codebase"
      - "Tool should conform to codebase, not vice versa"

    implementation:
      config_changes:
        - "autosummary_imported_members = False"
        - "html_static_path = [] (no custom static files)"
      docstring_fixes:
        - "factories.py: **kwargs RST markup"
        - "scenarios.py: topology diagram formatting"
        - "struggle.py: docstring emphasis"
        - "relationship.py: code block fencing"
        - "formulas.py: block quote formatting"
      module_path_fixes:
        - "docs/api/systems.rst: 6 incorrect module paths"
      exports_added:
        - "SolidaritySystem to __init__.py"
        - "TerritorySystem to __init__.py"

    consequences:
      positive:
        - "Zero Sphinx warnings"
        - "Python API surface unchanged"
        - "Import patterns preserved"
        - "CI now fails on doc warnings (-W flag)"
      negative:
        - "Must understand autosummary behavior for future docs"
        - "Some classes only documented in submodule docs"

    related_mantras:
      - "codebase_is_truth"
      - "differentiate_structural_issues"
      - "docstrings_enable_sphinx"

  ADR023_python_reexport_pattern:
    status: "accepted"
    date: "2025-12-11"
    title: "Python Re-Export Pattern in __init__.py is Intentional"
    context: |
      During Sphinx warning investigation, question arose:
      "Is the re-export pattern a ticking time bomb or just a Sphinx issue?"

      Re-export pattern in question:
      ```python
      # babylon/engine/__init__.py
      from babylon.engine.event_bus import Event
      from babylon.engine.simulation_engine import SimulationEngine
      __all__ = ["Event", "SimulationEngine", ...]
      ```

      Verified with Python runtime:
      ```python
      from babylon.engine import Event as Event1
      from babylon.engine.event_bus import Event as Event2

      Event1 is Event2  # True
      id(Event1) == id(Event2)  # True
      Event1.__module__  # 'babylon.engine.event_bus'
      ```

    decision: |
      KEEP the re-export pattern. This is standard Python API design.

      The re-export does NOT create copies. Python's import system
      ensures there is exactly ONE object in memory. The __init__.py
      re-export simply provides an alternative import path.

      Benefits:
      1. Cleaner imports: `from babylon.engine import Event`
      2. API stability: internal reorganization doesn't break imports
      3. Intentional public surface: __all__ declares what's "public"
      4. Industry standard: pandas, requests, pydantic all do this

    rationale:
      - "This is not duplication - it's aliasing"
      - "Same id(), same __module__, same object"
      - "Standard Python practice for public APIs"
      - "Removing re-exports would break existing imports"
      - "Sphinx issue is tooling, not architecture"

    verification_performed:
      - "Runtime object identity check (is operator)"
      - "Memory address check (id function)"
      - "Module attribute verification (__module__)"
      - "Major library pattern analysis"

    consequences:
      positive:
        - "Clean public API surface"
        - "Internal refactoring flexibility"
        - "Industry-standard pattern"
      negative:
        - "Sphinx needs configuration (ADR022)"
        - "Two import paths exist (may confuse new devs)"

    related_mantras:
      - "codebase_is_truth"
      - "differentiate_structural_issues"

  ADR024_documentation_system:
    status: "accepted"
    date: "2025-12-11"
    title: "Sphinx Documentation with RST and Autodoc"
    context: |
      Babylon needs documentation for:
      1. Human developers (API reference, guides)
      2. AI assistants (machine-readable context)
      3. Theoretical reference (MLM-TW concepts)

      Multiple documentation approaches evaluated.

    decision: |
      Tiered documentation system:

      **Tier 1: Sphinx (Human-Readable)**
      - Location: docs/
      - Format: RST with MyST for Markdown
      - Generation: Autodoc from docstrings
      - Output: HTML (local), potentially ReadTheDocs
      - CI: Build with -W (warnings fatal)

      **Tier 2: AI Docs (Machine-Readable)**
      - Location: ai-docs/
      - Format: YAML
      - Purpose: Context for AI assistants
      - Files: architecture, decisions, ontology, patterns, anti-patterns
      - Tokens: Optimized for context windows

      **Tier 3: Brainstorm (Ideas)**
      - Location: brainstorm/
      - Format: Markdown
      - Purpose: Design exploration, deferred ideas
      - Status: Not documentation, working notes

      **Docstring Standards**:
      - All public classes/functions: docstrings required
      - Format: RST-compatible (Sphinx autodoc)
      - Examples: Should pass doctest
      - Cross-references: :class:`Name`, :func:`name`

    rationale:
      - "Sphinx is Python standard, well-maintained"
      - "Autodoc keeps docs in sync with code"
      - "AI docs are token-efficient for context windows"
      - "Separation prevents docs from becoming stale"
      - "CI enforcement ensures docs build correctly"

    files:
      sphinx:
        - "docs/conf.py"
        - "docs/index.rst"
        - "docs/api/*.rst"
        - "docs/concepts/*.rst"
      ai_docs:
        - "ai-docs/*.yaml"
        - "ai-docs/README.md"
      brainstorm:
        - "brainstorm/mechanics/*.md"
        - "brainstorm/deferred-ideas.md"

    ci_integration:
      job: "docs"
      commands:
        - "poetry run pytest --doctest-modules src/babylon/systems/formulas.py"
        - "cd docs && poetry run sphinx-build -W -b html . _build/html"
      failure_mode: "Blocks merge"

    consequences:
      positive:
        - "Single source of truth (docstrings)"
        - "CI catches documentation issues"
        - "AI docs optimized for AI context"
        - "Clear separation of concerns"
      negative:
        - "Three documentation locations to maintain"
        - "RST formatting has learning curve"
        - "Docstrings must be kept Sphinx-compatible"

    related_mantras:
      - "docstrings_enable_sphinx"
      - "explain_why_not_what"

  ADR025_operation_clean_slate:
    status: "accepted"
    date: "2025-12-12"
    title: "Test Suite Refactor (Operation Clean Slate)"
    context: |
      The test suite grew to 1,500+ tests with increasing brittleness:
      - Fixture fragmentation: Same entities defined differently across test files
      - Assertion verbosity: Math-heavy assertions obscured test intent
      - Mock complexity: Smart mocks with calculation logic caused false positives

    decision: |
      Centralize test infrastructure via three tools:

      **1. DomainFactory** (tests/factories/domain.py)
      - Single source for entity creation
      - Sensible defaults, override only what matters
      - Methods: create_worker(), create_owner(), create_relationship(), create_world_state()

      **2. BabylonAssert** (tests/assertions.py)
      - Fluent domain-specific assertions
      - Classes: Assert, EntityAssert, RelationshipAssert
      - Transform assertions into political/economic narrative

      **3. MetricsCollectorProtocol** (src/babylon/metrics/interfaces.py)
      - Protocol for dependency injection
      - Dumb spy pattern: mocks record, don't calculate
      - Ensures mock/real implementation parity

    rationale:
      - "DomainFactory enables 'express only what differs' pattern"
      - "BabylonAssert transforms assertions into readable political narrative"
      - "Protocol pattern ensures mock/real implementation parity"

    consequences:
      positive:
        - "Tests are more readable and maintainable"
        - "Fixture changes propagate from single source"
        - "Rich error messages accelerate debugging"
        - "Mocks cannot diverge from production interface"
      negative:
        - "Learning curve for new testing patterns"
        - "Existing tests need gradual migration"

    implementation:
      files:
        - "tests/factories/domain.py"
        - "tests/assertions.py"
        - "src/babylon/metrics/interfaces.py"
      deprecates: "Raw dictionary creation for test entities"

    related_patterns:
      - "the_harvest"
      - "the_narrative_assertion"
      - "the_humble_mock"

  ADR030_teleological_pivot:
    status: "accepted"
    date: "2025-12-31"
    title: "The Teleological Pivot: From Stability Testing to Entropy Modeling"
    context: |
      The simulation paradigm had a fundamental philosophical error: it treated
      imperial stability as the default state and tested whether the system could
      survive. This produced "Eden Mode" simulations where:

      1. **Existence was free**: base_subsistence = 0.0 meant entities persisted
         without metabolic cost (zombie states)
      2. **Earth was infinite**: No hysteresis in biocapacity degradation meant
         extraction had no permanent consequences
      3. **Success = Survival**: Optimization targeted maximizing ticks_survived
         rather than realistic decay patterns

      This contradicts historical materialism. Imperialism is inherently unstable;
      it cannot find equilibrium. The question is not whether the empire survives,
      but HOW it dies.

      The Material Reality Refactor (VitalitySystem + ProductionSystem) enabled
      entity death but didn't address the teleological question: what should the
      simulation demonstrate?

    decision: |
      **The Tragedy of Inevitability: Default state is COLLAPSE, not stability.**

      1. **The 20-Year Entropy Standard**
         - Extend simulation horizon from 52 ticks (1 year) to 1040 ticks (20 years)
         - Success = Realistic Decay, not survival
         - Ideal death tick: 800-900 (Year 15-17)
         - Ideal decay shape: gentle downward slope, accelerating in final 200 ticks

      2. **The Calorie Check (Mandatory Invariant)**
         - base_subsistence > 0.0 ALWAYS
         - Existence costs calories; free existence is banned
         - All scenario factories must assert this

      3. **Hysteresis Mechanism**
         - Extraction causes PERMANENT damage to max_biocapacity
         - Formula: max_B_new = max_B - (extraction × hysteresis_rate)
         - The Metabolic Rift is irreversible

      4. **TRPF (Tendency of Rate of Profit to Fall)**
         - imperial_rent_pool must show declining growth rate over decades
         - Equilibrium stability is an anti-pattern

      5. **Banned Concepts**
         - Infinite biocapacity (Eden Mode)
         - Regeneration > Extraction under capitalism
         - Free existence (base_subsistence = 0)
         - Equilibrium stability
         - Player "victory" through survival

    rationale:
      historical_materialism: |
        Marx identified the Metabolic Rift - capitalism's fundamental incompatibility
        with ecological sustainability. The simulation must embody this: extraction
        externalizes regeneration costs, widening the rift with each cycle.

      anti_accelerationism: |
        Players cannot prevent collapse. They can only:
        1. Accelerate it (revolutionary action)
        2. Decelerate it (system maintenance - delays inevitable)
        3. Shape its character (revolutionary vs fascist resolution)

      educational_purpose: |
        The simulation is not about winning. It is about understanding.
        When biocapacity falls below consumption needs, someone dies.
        The choice of WHO is political - socialist vs fascist resolution.

      gameplay_quality: |
        Eden Mode produced flatline graphs - boring and unrealistic.
        The 20-Year Entropy Standard ensures visible dynamics:
        - Wealth lines diverge (periphery impoverishment visible)
        - Biocapacity depletes (ecological crisis emerges)
        - Death occurs (stakes are real)

    implementation:
      documentation_updated:
        - "ai-docs/tuning-standard.md (v2.0.0 - complete rewrite)"
        - "ai-docs/theory.md (added Tragedy of Inevitability section)"
        - "ai-docs/balance-targets.yaml (added entropy_targets section)"
        - "ai-docs/metabolic-slice.yaml (added hysteresis section)"

      objective_function_v2:
        components:
          death_timing: "40% weight - death should occur tick 800-900"
          decay_smoothness: "30% weight - gentle downward slope"
          trpf_manifestation: "20% weight - declining rate of profit"
          biocapacity_exhaustion: "10% weight - near-zero at death"

      anti_patterns_defined:
        flatline: "std(wealth) < 0.01 - zombie state"
        cliff: "death_tick < 100 - too aggressive"
        eternal_empire: "wealth[1000]/wealth[0] > 0.9 - Eden Mode"
        hollow_stability: "mean(wealth[500:1000]) ≈ mean(wealth[0:500])"

      new_parameters:
        entropy_factor: "[1.1, 1.5] - extraction inefficiency"
        hysteresis_rate: "[0.001, 0.01] - permanent max_biocapacity damage"
        trpf_coefficient: "[0.0001, 0.001] - rate of profit decay"
        rent_pool_decay: "[0.001, 0.005] - background rent evaporation"

    consequences:
      positive:
        - "Simulations now show visible dynamics, not flatlines"
        - "Ecological crisis emerges naturally from extraction"
        - "Death creates real stakes and meaningful choices"
        - "Aligns with historical materialist theory"
        - "20-year horizon reveals TRPF and generational effects"
        - "Player agency constrained to realistic options"

      negative:
        - "Longer simulation runs (~10-20 minutes for full 1040 ticks)"
        - "Existing parameter tuning must be recalibrated"
        - "Some tests may need threshold adjustments"
        - "Players cannot 'win' in traditional sense"

    theoretical_grounding:
      marx_metabolic_rift: "Capital Vol. 1 - separation of town and country"
      foster_metabolic_rift: "Marx's Ecology - irreversible ecological damage"
      trpf: "Capital Vol. 3 - tendency of rate of profit to fall"

    mantra: "The question is not whether the empire falls. The question is how."

    supersedes:
      - "52-tick (1-year) simulation standard"
      - "Survival-maximizing objective function"
      - "Eden Mode parameter configurations"

    related_decisions:
      - "ADR020_parameter_tuning_methodology"
      - "ADR016_fascist_bifurcation"

  ADR031_test_constants_architecture:
    status: "accepted"
    date: "2025-12-31"
    title: "Centralized Test Constants with Frozen Dataclasses"
    context: |
      During Phase 3-4 development, the test suite accumulated ~1,500+ tests across
      21 test files with pervasive magic numbers:

      ```python
      # Before: Magic numbers scattered across tests
      assert worker.wealth == 10.0  # What does 10.0 mean?
      assert edge.tension == 0.5    # Threshold? Midpoint? Default?
      component = VitalityComponent(population=100.0, subsistence_needs=8.0)
      ```

      Problems with magic numbers:
      1. **Semantic Opacity**: 0.5 could be probability, ideology, consciousness, ratio
      2. **Duplicate Definitions**: Same values defined differently across files
      3. **Maintenance Burden**: Changing a default requires finding all occurrences
      4. **Test Intent Obscured**: Numbers distract from behavioral testing

      The question: How do we extract these values without over-engineering?

    decision: |
      **Centralize all test magic numbers in `tests/constants.py` using frozen dataclasses.**

      Structure:
      ```python
      @dataclass(frozen=True)
      class WealthDefaults:
          """Currency values [0, inf) for test entities."""
          __test__ = False  # Prevent pytest collection
          DEFAULT_WEALTH: float = 10.0
          WORKER_BASELINE: float = 0.5
          OWNER_BASELINE: float = 10.0
          SIGNIFICANT: float = 100.0
          LARGE: float = 1_000_000.0

      @dataclass(frozen=True)
      class TestConstants:
          """Single entry point for all test constants."""
          __test__ = False
          Wealth: WealthDefaults = field(default_factory=WealthDefaults)
          Probability: ProbabilityDefaults = field(default_factory=ProbabilityDefaults)
          # ... other categories
      ```

      Import pattern:
      ```python
      from tests.constants import TestConstants
      TC = TestConstants

      def test_worker_default_wealth(self) -> None:
          worker = SocialClass(...)
          assert worker.wealth == TC.Wealth.DEFAULT_WEALTH  # Semantic!
      ```

      **Key Distinction: Domain Constants vs Type Boundaries**

      EXTRACT to constants (domain-specific):
      - Default values (DEFAULT_WEALTH = 10.0)
      - Threshold values (AWAKENING = 0.7)
      - Scenario values (PERIPHERY_WORKER = 20.0)
      - Config defaults (EXTRACTION_EFFICIENCY = 0.8)

      KEEP INLINE (type contract testing):
      - Boundary values (0.0, 1.0 for Probability type)
      - Edge cases (-0.001 for "just below zero")
      - Precision tests (0.123456789 for quantization)

      Rationale: Type boundary tests verify the TYPE DEFINITION itself.
      The values 0.0 and 1.0 ARE the Probability type contract.
      Extracting them to constants would reduce clarity.

    rationale:
      frozen_dataclass:
        benefit: "Immutable, cannot be accidentally modified during tests"
        benefit: "IDE autocomplete via dot notation"
        benefit: "Type hints propagate through"

      value_type_organization:
        benefit: "Same semantic value used consistently (0.5 = Probability.MIDPOINT)"
        benefit: "Aligns with production type system (Currency, Probability, Ideology)"
        benefit: "Reduces conceptual mapping overhead"
        rejected: "Per-model organization would duplicate values across categories"

      pytest_collection_prevention:
        pattern: "__test__ = False on all dataclasses"
        reason: "Pytest auto-collects classes with 'Test' in name"
        without_it: "TypeError: 'WealthDefaults' object is not iterable"

      tc_alias:
        pattern: "TC = TestConstants"
        benefit: "Concise usage: TC.Wealth.DEFAULT vs TestConstants.Wealth.DEFAULT"
        convention: "Uppercase signals 'constant namespace'"

    implementation:
      location: "tests/constants.py"
      test_count: 928 tests migrated across 21 files

      categories_implemented:
        - "WealthDefaults: Currency values [0, inf)"
        - "ProbabilityDefaults: Probability values [0, 1]"
        - "IdeologyDefaults: Ideology values [-1, +1]"
        - "ConsciousnessDefaults: Consciousness values [0, 1]"
        - "ThresholdsDefaults: Crossover/rupture thresholds"
        - "VitalityDefaults: Population and subsistence values"
        - "OrganizationDefaults: Cohesion and cadre values"
        - "SpatialDefaults: Mobility values"
        - "AgitationDefaults: George Jackson model values [0, inf)"
        - "RevolutionaryFinanceDefaults: War chest, burn rate, income sources"
        - "EconomicFlowDefaults: Rent pool, extraction flows"
        - "MetabolicRiftDefaults: Entropy factor values"
        - "MarxCapitalExamples: Capital Vol. 1 numerical examples"
        - "TRPFDefaults: Rate of profit fall parameters"

      phases_completed:
        phase_1: "Formula tests (survival_calculus, bourgeoisie_decision, etc.)"
        phase_2: "Engine tests (simulation_engine, imperial_circuit)"
        phase_3: "Component tests (vitality, ideological, material, etc.)"
        phase_4: "Model tests (metrics, graph_models, revolutionary_finance)"

    consequences:
      positive:
        - "Semantic clarity: TC.Consciousness.AWAKENING vs 0.7"
        - "Single source of truth for test values"
        - "IDE autocomplete aids discovery"
        - "Frozen dataclasses prevent accidental mutation"
        - "Type boundaries remain inline where appropriate"
        - "Grep-friendly: can find all uses of a constant"

      negative:
        - "Initial migration effort (~4-6 hours)"
        - "New contributors must learn constant namespace"
        - "Some values legitimately belong inline (boundary tests)"
        - "Import boilerplate in each test file"

    anti_patterns:
      over_extraction: |
        DON'T extract type boundary values to constants:
        ```python
        # BAD: Obscures what's being tested
        assert Probability(TC.Probability.BOUNDARY_ZERO) is valid

        # GOOD: Boundary is self-documenting
        assert Probability(0.0) is valid  # Lower bound of [0, 1]
        ```

      under_organization: |
        DON'T create per-test-file constant files:
        ```python
        # BAD: Fragments knowledge
        from tests.unit.models.test_social_class_constants import WORKER_WEALTH

        # GOOD: Centralized, discoverable
        from tests.constants import TestConstants
        ```

    related_decisions:
      - "ADR025_operation_clean_slate (test infrastructure refactor)"
      - "ADR006_pydantic_models (constrained types)"

    mantra: "Name the number. Test the behavior."

  ADR032_materialist_causality_system_order:
    status: "accepted"
    date: "2025-12-31"
    title: "Materialist Causality System Execution Order"
    context: |
      The simulation engine executes systems in sequence, with each system seeing
      mutations from all previous systems. The original order did not strictly
      enforce materialist causality:

      Old order (problems):
      1. VitalitySystem
      2. ProductionSystem
      3. ImperialRentSystem
      4. SolidaritySystem
      5. ConsciousnessSystem  ← Ideology before material conditions resolved
      6. SurvivalSystem
      7. StruggleSystem
      8. ContradictionSystem
      9. TerritorySystem    ← Land conditions computed AFTER production!
      10. MetabolismSystem  ← Ecology at the very end

      Issues:
      - Territory runs after production (violates: land affects production)
      - Consciousness runs before survival calculus (ideology before material)
      - Metabolism runs last (ecological limits should constrain production)
      - Subsistence burn was embedded in ImperialRentSystem

    decision: |
      **Reorder systems to enforce strict materialist causality: base before superstructure.**

      New order with rationale:

      **Material Base (biological, spatial, economic):**
      1. VitalitySystem - Dead entities don't work (The Drain + The Reaper)
      2. TerritorySystem - Land conditions affect production (Carceral Geography)
      3. ProductionSystem - Value creation from labor × biocapacity (The Labor)
      4. SolidaritySystem - Organization affects bargaining power (Political Organization)
      5. ImperialRentSystem - Value extraction (The Extraction)
      6. MetabolismSystem - Ecological residue of production (The Consequence)

      **Superstructure (social, ideological):**
      7. SurvivalSystem - Risk assessment from material state (P(S|A), P(S|R))
      8. StruggleSystem - Agency responds to survival odds (George Floyd Dynamic)
      9. ConsciousnessSystem - Ideology responds to material conditions (Bifurcation)
      10. ContradictionSystem - Final systemic tension accounting (The Reckoning)

      **Key architectural change:**
      - Subsistence burn moved from ImperialRentSystem._process_subsistence_phase()
        to VitalitySystem.step() (two-phase: The Drain + The Reaper)

    rationale:
      base_before_superstructure:
        principle: "Material conditions must be computed before social/ideological effects"
        example: "Dead workers can't produce value; ideology responds to material state"

      territory_before_production:
        principle: "Land conditions (biocapacity, heat, eviction) affect production capacity"
        example: "Degraded land produces less value"

      metabolism_after_extraction:
        principle: "Ecological degradation is the CONSEQUENCE of extraction, not the cause"
        example: "Extraction depletes biocapacity for next tick"

      subsistence_in_vitality:
        principle: "Cost of living is biological, not economic"
        example: "Elites with high multipliers burn faster without income"

    consequences:
      positive:
        - "Strict materialist causality in simulation"
        - "Subsistence burn happens before economic production"
        - "Territory degradation affects production on same tick"
        - "Ideology drift responds to fully resolved material state"
        - "Clear separation: base (1-6) vs superstructure (7-10)"
      negative:
        - "TerritorySystem now runs early, may affect performance"
        - "ConsciousnessSystem runs later, may change behavior"
        - "Existing tests may need adjustment for new timing"

    tests_added:
      - "tests/unit/engine/test_system_order.py::TestMaterialistCausalityOrder"
      - "tests/unit/engine/systems/test_vitality.py::TestVitalitySubsistenceBurn"

    files_modified:
      - "src/babylon/engine/simulation_engine.py (reordered _DEFAULT_SYSTEMS)"
      - "src/babylon/engine/systems/vitality.py (added subsistence burn)"
      - "src/babylon/engine/systems/economic.py (deprecated subsistence phase)"

    related_decisions:
      - "ADR001_embedded_trinity (base/relations/superstructure)"
      - "ADR004_system_architecture (modular systems)"

  ADR033_mass_line_refactor:
    status: "accepted"
    date: "2025-12-31"
    title: "Mass Line: Agent-as-Block Population Dynamics"
    context: |
      The original simulation modeled each SocialClass as a single entity with
      binary alive/dead states. This prevented modeling of:
      - Intra-class inequality (some workers starving while others survive)
      - Gradual population decline (grinding attrition vs sudden death)
      - Demographic dynamics (Malthusian equilibrium)

      The "Hump Shape" crisis of social reproduction requires modeling how
      inequality within a class causes marginal workers to starve even when
      average wealth is sufficient.

    decision: |
      **Transform simulation from Agent-as-Person to Agent-as-Block.**

      Add two new fields to SocialClass:
      - `population: int` - Block size (default=1 for backward compatibility)
      - `inequality: Gini` - Intra-class inequality coefficient [0,1]

      Implement "Grinding Attrition Formula" in VitalitySystem Phase 2:
      ```
      per_capita_wealth = wealth / population
      marginal_wealth = per_capita_wealth × (1 - inequality)
      mortality_rate = max(0, (consumption_needs - marginal_wealth) / consumption_needs)
      deaths = floor(population × mortality_rate × base_mortality_factor)
      ```

      **Malthusian Correction**: Deaths reduce population → per-capita wealth
      increases → future mortality decreases → equilibrium at carrying capacity.

    rationale:
      backward_compat:
        principle: "Default population=1, inequality=0 preserves old behavior"
        example: "Existing scenarios continue to work without modification"

      theoretical_basis:
        principle: "Intra-class inequality affects survival differentially"
        example: "High Gini means marginal workers starve even with sufficient avg wealth"

      malthusian_dynamics:
        principle: "Natural equilibrium as population adjusts to carrying capacity"
        example: "Deaths relieve per-capita scarcity, reducing future mortality"

      crisis_modeling:
        principle: "Enables 'Hump Shape' crisis of social reproduction"
        example: "Growing inequality eventually causes population decline"

    consequences:
      positive:
        - "Richer demographic modeling"
        - "Enables scenarios with population growth/decline"
        - "More realistic inequality effects"
        - "Malthusian equilibrium dynamics"
        - "Backward compatible with existing scenarios"
      negative:
        - "Additional complexity in VitalitySystem"
        - "New event type (POPULATION_DEATH) requires observer updates"
        - "Tests marked @pytest.mark.red_phase during development"

    tests_added:
      - "tests/unit/engine/systems/test_vitality.py::TestGrindingAttrition"

    files_modified:
      - "src/babylon/models/types.py (added Gini type)"
      - "src/babylon/models/entities/social_class.py (added population, inequality)"
      - "src/babylon/config/defines.py (added VitalityDefines)"
      - "src/babylon/models/enums.py (added POPULATION_DEATH event)"
      - "src/babylon/engine/systems/vitality.py (3-phase with Grinding Attrition)"

    related_decisions:
      - "ADR032_materialist_system_order (VitalitySystem runs first)"
      - "ADR001_embedded_trinity (material conditions determine survival)"

  ADR034_scenario_layer_test_taxonomy:
    status: "accepted"
    date: "2026-01-01"
    title: "Scenario Layer for Long-Trajectory Tests"
    context: |
      Integration tests were organized into mechanics/, system/, and ui/ subdirectories
      (per the 2025-01-01 taxonomy refactor). However, tests validating long-term
      simulation trajectories (50-70 year timescales) require distinct treatment:

      - They test theoretical outcomes, not implementation correctness
      - They run for thousands of ticks (slow, multi-minute runtime)
      - They validate the "null hypothesis" (default trajectory without player intervention)
      - They demonstrate adherence to ai-docs/carceral-equilibrium.md and ai-docs/theory.md

      The existing test categories (mechanics, system, ui) don't adequately capture
      this distinction between "does the code work?" and "does the simulation
      produce theoretically expected trajectories?"

    decision: |
      Create a dedicated `tests/scenarios/` directory for trajectory validation tests.

      These tests:
      1. Focus on phase sequence ordering, not exact tick timing
      2. Validate canonical outcomes from theoretical documents
      3. Are marked with @pytest.mark.slow for CI exclusion from fast feedback loops
      4. Test the MVP: simulation behavior in absence of player intervention

      Move existing trajectory-focused tests from integration/:
      - test_endgame_flow.py → tests/scenarios/
      - test_fascist_bifurcation.py → tests/scenarios/

      Add new @pytest.mark.slow marker for long-running scenario tests.

    rationale:
      separation_of_concerns:
        principle: "Scenario tests verify theory, not implementation"
        example: "Carceral equilibrium test validates 70-year arc from theory docs"

      ci_performance:
        principle: "Long tests excluded from fast CI feedback loop"
        example: "@pytest.mark.slow skipped by default, run nightly"

      documentation_alignment:
        principle: "Tests serve as executable specification of theory docs"
        example: "test_carceral_equilibrium.py validates ai-docs/carceral-equilibrium.md"

      taxonomy_clarity:
        principle: "Four-tier test structure maps to distinct concerns"
        example: |
          - mechanics/: MLM-TW game theory verification
          - system/: Engine architecture validation
          - ui/: Presentation layer testing
          - scenarios/: Long-term trajectory validation

    consequences:
      positive:
        - "Clear taxonomy: mechanics (theory), system (plumbing), ui (presentation), scenarios (trajectories)"
        - "Slow tests isolated with dedicated marker"
        - "Theoretical claims become testable assertions"
        - "MVP defined as verifiable behavior (null hypothesis trajectory)"
      negative:
        - "Additional test category to maintain"
        - "Scenario tests may need retuning if parameters change"
        - "Long test runs require dedicated CI time"

    tests_added:
      - "tests/scenarios/test_carceral_equilibrium.py::TestCarceralEquilibrium"
      - "@pytest.mark.slow marker registered in pyproject.toml"

    files_modified:
      - "pyproject.toml (added 'slow' marker)"
      - "tests/scenarios/__init__.py (new directory)"
      - "tests/scenarios/conftest.py (imperial circuit fixtures)"
      - "tests/scenarios/test_carceral_equilibrium.py (70-year arc test)"

    files_moved:
      - "tests/integration/system/test_endgame_flow.py → tests/scenarios/"
      - "tests/integration/mechanics/test_fascist_bifurcation.py → tests/scenarios/"

    related_decisions:
      - "ADR001_embedded_trinity (theory docs as architectural source of truth)"
      - "ADR003_ai_as_observer (deterministic mechanics for reproducible outcomes)"

  ADR035_namespace_driven_mise_tasks:
    status: "accepted"
    date: "2026-01-01"
    title: "Namespace-Driven Mise Task Convention"
    context: |
      The .mise.toml task configuration had grown organically with inconsistent naming:
      - Some tasks used hyphens: test-fast, analyze-trace
      - Some used colons: qa:audit, analyze:matrix
      - Some had no namespace: tune, map, dashboard
      - Duplicate aliases existed for the same functionality

      This made task discovery difficult and the interface inconsistent. Phase 3
      cleanup required a systematic approach to task organization.

    decision: |
      Adopt a colon-namespaced task naming convention:
        check         - Fast CI gate (lint + format + typecheck + test:unit)
        test:*        - Test runners by scope (unit, int, scenario, all)
        sim:*         - Simulation execution and analysis
        tune:*        - Parameter optimization tools
        qa:*          - Quality assurance and verification
        demo:*        - Integration demos and persona testing
        data:*        - Data pipeline operations
        docs:*        - Documentation building
        mutate:*      - Mutation testing
        midi:*        - Music generation

      Clean break from old names (no backward-compatible aliases).
      Update CLAUDE.md and ai-docs/tooling.yaml to document new structure.

    rationale:
      discoverability:
        principle: "Namespaces group related functionality"
        example: "'mise tasks' shows logical groupings, 'mise run test:' tab-completes"

      consistency:
        principle: "Single naming convention reduces cognitive load"
        example: "All test tasks start with test:, all tune tasks start with tune:"

      clean_interface:
        principle: "Remove duplicate aliases and legacy names"
        example: "No 'map' alias for 'tune:landscape', no 'audit' alias for 'qa:audit'"

    consequences:
      positive:
        - "Clear discoverability via namespace prefixes"
        - "Tab completion works naturally (mise run test:<tab>)"
        - "Documentation accurately reflects available commands"
        - "No confusion from multiple names for same task"
      negative:
        - "Breaking change for muscle memory (old names won't work)"
        - "CI workflows may need updating if referencing old task names"

    files_created:
      - "tools/profiler.py (cProfile wrapper for sim:profile)"

    files_deleted:
      - "tools/init_epoch2_db.py (legacy Epoch 2 database init)"

    files_modified:
      - ".mise.toml (complete rewrite with namespace structure)"
      - "CLAUDE.md (Commands section updated)"
      - "ai-docs/tooling.yaml (mise section updated)"

    related_decisions:
      - "ADR034_scenario_layer_test_taxonomy (introduced test:scenario namespace)"

  ADR036_shared_utilities_module:
    status: "accepted"
    date: "2026-01-01"
    title: "Centralized tools/shared.py for simulation utilities"

    context: |
      Multiple tools duplicated inject_parameter() and is_dead() functions.
      parameter_analysis.py re-implemented utilities instead of importing from
      tune_parameters.py. Entity ID constants (C001, C002, etc.) were defined
      redundantly across multiple files. This created maintenance burden and
      divergence risk - a fix in one file wouldn't propagate to others.

    decision: |
      Create tools/shared.py as single source of truth for:
      - inject_parameter(), inject_parameters(): Parameter injection into GameDefines
      - is_dead(), is_dead_by_wealth(): VitalitySystem-aligned death detection
      - run_simulation(): Standard simulation runner returning metrics dict
      - get_tunable_parameters(): Parameter enumeration for SALib/Optuna
      - Entity ID constants: PERIPHERY_WORKER_ID, COMPRADOR_ID, etc.
      - Column prefix mappings: ENTITY_COLUMN_PREFIX for CSV output

      All tools must import from shared.py, never re-implement or duplicate.
      tune_parameters.py re-exports for backwards compatibility.

    rationale:
      DRY:
        principle: "Don't Repeat Yourself"
        benefit: "One place to update death detection logic, parameter injection, etc."

      testability:
        principle: "Shared module can have dedicated unit tests"
        benefit: "Core utilities verified once, not duplicated test coverage"

      discoverability:
        principle: "New tools find utilities in one place"
        benefit: "Lower barrier to creating new analysis tools"

      consistency:
        principle: "All tools use identical implementations"
        benefit: "No subtle behavioral differences across tools"

    consequences:
      positive:
        - "Single source of truth for simulation utilities"
        - "Easier to add new shared functionality (e.g., inject_parameters for batch)"
        - "Consistent behavior across all tools"
        - "Clear import hierarchy: shared.py <- all other tools"
      negative:
        - "Circular import risk (mitigated by clean dependency order)"
        - "One more file to maintain (but net reduction in total code)"

    files_created:
      - "tools/shared.py (centralized utilities)"
      - "tools/monte_carlo.py (Monte Carlo UQ)"
      - "tools/sensitivity_analysis.py (SALib Morris/Sobol)"
      - "tools/regression_test.py (baseline comparison)"
      - "tests/baselines/ (regression baseline directory)"

    files_modified:
      - "tools/tune_parameters.py (imports from shared, re-exports for compat)"
      - "tools/parameter_analysis.py (removed duplicated functions)"
      - "tools/landscape_analysis.py (import from shared)"
      - "tools/audit_simulation.py (import from shared)"
      - "tools/tune_agent.py (import from shared)"
      - ".mise.toml (6 new tasks added)"
      - "pyproject.toml (scipy, SALib dependencies)"
      - "CLAUDE.md (Simulation Lab section expanded)"

    related_decisions:
      - "ADR031_test_constants_centralized (similar pattern for test values)"
      - "ADR035_namespace_driven_mise_tasks (task organization)"

  ADR037_bourgeoisie_wealth_concentration:
    status: "deferred"
    deferred_to: "Epoch 2"
    date: "2026-01-01"
    title: "Branching trajectories: Classical Concentration vs Warlord Coup"
    original_severity: "CRITICAL BUG"
    discovered_during: "Epoch 1 MVP - necropolis_viewer.py 100-year simulation"

    key_insight: |
      The current math IS fundamentally flawed (bourgeoisie should accumulate).
      BUT the observed dynamics ALSO model a valid historical trajectory where
      enforcers seize power from a dying bourgeoisie.

      Epoch 2 will implement BOTH as branching trajectories:
      - Trajectory A: Fix the math → Classical Concentration (bourgeoisie survives)
      - Trajectory B: Keep the dynamics → Warlord Coup (enforcers seize power)

      The branch depends on enforcer loyalty, which depends on payment consistency.

    warlord_trajectory: |
      The "bug" models enforcers seizing power when bourgeoisie can no longer pay.
      Key insight: Bourgeoisie control means of violence BY PROXY through money.

      - C_b pays enforcers → enforcers control tanks, drones, helicopters
      - When C_b's money runs out → enforcers still have the weapons
      - Coup opportunity: Enforcers become the new ruling class (warlords)

      Historical parallels:
      - Roman Praetorian Guard (killed emperors, installed own candidates)
      - Chile 1973 (Pinochet), Argentina 1976 (Videla), Myanmar 2021
      - Every military junta in history

      Branching condition:
        enforcer_loyalty = f(C_b_wealth, payment_consistency, ideology)
        If C_b can pay → loyal → Trajectory A (necropolis ruled by capitalists)
        If C_b cannot pay → coup → Trajectory B (necropolis ruled by warlords)

    context: |
      Running the 100-year Carceral Equilibrium simulation revealed a fundamental
      flaw in wealth dynamics. The simulation shows:

        Year 0:   C_b wealth = 1.10    LA wealth = 9.62
        Year 100: C_b wealth = 0.01    LA wealth = 15,954.87

      This is backwards. The bourgeoisie should CONCENTRATE wealth during crisis,
      not disappear. The Labor Aristocracy should be SQUEEZED, not accumulate.

      The current model treats LA as the accumulating class, which contradicts
      basic Marxist political economy. Capital concentrates upward, not outward.

    the_correct_model: |
      The bourgeoisie (C_b) and Labor Aristocracy (LA) form an ALLIANCE based on
      shared exploitation of the periphery. Think of it as a mob organization:

        - C_b = The Boss (Tony Soprano)
          Owns the means of production, extracts surplus value from everyone,
          takes the lion's share of imperial plunder.

        - LA = The Underboss / Made Men
          Junior partners who get a generous cut in exchange for:
          - Political loyalty (voting for imperialist parties)
          - Cultural hegemony (reproducing bourgeois ideology)
          - Enforcement (cops, soldiers, prison guards, scabs)

        - Enforcers = The Soldiers
          Former LA who become the enforcement arm when crisis hits.
          Loyal henchmen who maintain their status through violence.

        - Internal Proletariat = Those who get whacked
          Former LA who fall out of favor, marked for elimination.

      The alliance holds as long as imperial rent flows. When it stops, the
      Boss (C_b) betrays the Underboss (LA) to maintain profit rates.

    current_bug: |
      The ImperialRentSystem treats C_b as a pass-through, not an accumulator:

        Periphery → [EXPLOITATION] → Comprador → [TRIBUTE] → C_b → [WAGES] → LA
                                                                ↓
                                                          (rent pool)

      C_b pays super-wages from a "rent pool" until exhausted, then... dies?
      Meanwhile LA accumulates the wages without any extraction flowing back up.

      Missing: EXPLOITATION edge from LA to C_b (surplus value extraction).

    required_fix: |
      1. ADD: EXPLOITATION edge (LA → C_b)
         Every tick, C_b extracts surplus value from LA labor:
           surplus = (LA.population × labor_productivity) - wages_paid
           C_b.wealth += surplus

      2. MODIFY: Super-wages as SHARE, not FULL VALUE
         Super-wages mean LA gets paid ABOVE SUBSISTENCE, not above value.
         The bourgeoisie ALWAYS extracts surplus, even from privileged workers.

      3. ADD: Concentration dynamics during crisis
         When imperial rent stops:
           - C_b squeezes LA harder (wage cuts, union busting)
           - C_b wealth ACCELERATES (crisis = concentration)
           - LA wealth DECLINES (middle class death spiral)

      4. ADD: C_b funds carceral apparatus
         Enforcers are paid BY C_b, not self-sustaining.
         C_b extracts prison labor value from Internal Proletariat.

    expected_dynamics: |
      Phase 1: Imperial Extraction (Years 0-40)
        C_b: GROWS (imperial rent + domestic surplus extraction)
        LA:  GROWS SLOWLY (super-wages, but still exploited)

      Phase 2: Peripheral Revolt (Years 20-30)
        C_b: STILL GROWS (squeezes LA harder)
        LA:  STAGNATES then DECLINES

      Phase 3: Crisis (Years 30-50)
        C_b: ACCELERATES (crisis = concentration)
        LA:  COLLAPSES (wage cuts, unemployment, debt)

      Phase 4: Carceral Turn (Years 40-60)
        C_b: CONTINUES GROWING (prison labor extraction)
        Enforcers: STABLE (bribed guards)
        Prisoners: ZERO WEALTH (pure extraction)

      Phase 5: Stable Necropolis (Years 60+)
        C_b: MAXIMUM CONCENTRATION
        Reduced population serves concentrated capital.

    rationale:
      marxist_theory: |
        - Capital concentrates (Marx, Capital Vol. 1)
        - Crisis accelerates concentration, not dispersal
        - The bourgeoisie is the LAST class to suffer, not the first
      mlm_tw_theory: |
        - LA are junior partners, not independent accumulators
        - Super-wages are bribes from imperial rent, not ownership
        - When rent stops, the alliance breaks and LA gets squeezed
      historical_evidence: |
        - 2008 crisis: Banks got bailouts, workers got foreclosures
        - COVID: Billionaires doubled wealth, workers died
        - Every crisis: Wealth concentrates upward, not outward

    consequences:
      positive:
        - "Correct materialist dynamics: capital concentrates"
        - "Accurate class alliance model (boss + underboss)"
        - "Realistic crisis dynamics (LA squeezed, C_b thrives)"
        - "Enforcers as bought loyalty, not self-sustaining class"
      negative:
        - "Requires rework of ImperialRentSystem edge structure"
        - "May affect tuned parameters (Optuna results)"
        - "More complex wealth flow calculations"

    implementation_priority: "Epoch 2 (before player agency)"

    files_to_modify:
      - "src/babylon/engine/systems/economic.py (add EXPLOITATION LA→C_b)"
      - "src/babylon/engine/scenarios.py (update edge structure)"
      - "src/babylon/models/enums.py (may need new EdgeType)"
      - "tests/scenarios/test_carceral_equilibrium.py (update expectations)"
      - "tools/carceral_scoring.py (add wealth concentration metrics)"

    related_decisions:
      - "ADR001_embedded_trinity (material base determines outcomes)"
      - "ADR032_materialist_causality_system_order (systems run in order)"

# =============================================================================
# REJECTED DECISIONS
# =============================================================================

rejected:

  RDR001_postgres:
    date: "2024-12-07"
    proposal: "Use PostgreSQL for persistence"
    rejection_reason: "Overkill for local single-player game"
    replaced_by: "ADR002_sqlite_over_postgres"

  RDR002_toml_data:
    date: "2024-12-07"
    proposal: "Use TOML for game data"
    rejection_reason: |
      JSON has better schema validation ecosystem.
      TOML better for config, JSON better for data.
    note: "TOML may still be used for configuration files"
