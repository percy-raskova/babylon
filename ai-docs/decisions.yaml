# Babylon Architecture Decisions
# Key decisions and their rationale (ADR-style)

meta:
  version: "1.5.0"
  updated: "2025-12-11"
  format: "decision_id: {status, date, context, decision, rationale, consequences}"

# =============================================================================
# ARCHITECTURAL DECISIONS
# =============================================================================

decisions:

  ADR001_embedded_trinity:
    status: "accepted"
    date: "2024-12"
    title: "Three-layer local architecture (Ledger/Topology/Archive)"
    context: |
      Game needs to handle rigid state (economics), fluid state (relationships),
      and semantic memory (history/narrative) differently.
    decision: |
      Separate into three architectural layers:
      - Ledger: SQLite/Pydantic for material state
      - Topology: NetworkX for relational state
      - Archive: ChromaDB for semantic memory
    rationale:
      - "Each layer has different consistency/mutability requirements"
      - "Separation enables independent testing and evolution"
      - "Mirrors materialist distinction: base/relations/superstructure"
    consequences:
      positive:
        - "Clear separation of concerns"
        - "Can swap implementations per layer"
      negative:
        - "More complexity than single-store approach"
        - "Cross-layer queries require coordination"

  ADR002_sqlite_over_postgres:
    status: "accepted"
    date: "2024-12-07"
    title: "SQLite instead of PostgreSQL"
    context: |
      Initially configured PostgreSQL, but this is a local game
      with no multi-user requirements.
    decision: "Use SQLite for all persistent storage"
    rationale:
      - "No server to manage"
      - "Zero configuration"
      - "Portable game saves (single file)"
      - "Sufficient performance for single-player"
    consequences:
      positive:
        - "Simpler deployment"
        - "No external dependencies"
      negative:
        - "Can't easily add multiplayer later"
        - "No concurrent writes (not needed)"

  ADR003_ai_as_observer:
    status: "accepted"
    date: "2024-12"
    title: "AI generates narrative, never controls mechanics"
    context: |
      LLMs are non-deterministic. Game mechanics must be
      reproducible and testable.
    decision: |
      AI sits in Archive layer only. It can:
      - Query state and history
      - Generate narrative text
      It cannot:
      - Modify game state
      - Determine mechanical outcomes
    rationale:
      - "Deterministic mechanics enable testing"
      - "Reproducible gameplay"
      - "AI failures don't break game"
      - "Separation of narration from simulation"
    consequences:
      positive:
        - "Game works without AI"
        - "Testable mechanics"
      negative:
        - "Less emergent AI behavior"
        - "More explicit mechanics required"

  ADR004_json_over_xml:
    status: "accepted"
    date: "2024-12-07"
    title: "JSON for game data instead of XML"
    context: |
      Legacy data was in XML. Need machine-readable format
      that's easy to validate and work with.
    decision: "Migrate all game data to JSON with JSON Schema validation"
    rationale:
      - "Native Python support (json module)"
      - "JSON Schema provides validation"
      - "Easier to read/write programmatically"
      - "Better tooling ecosystem"
    consequences:
      positive:
        - "Schema validation catches errors early"
        - "Cleaner data structures"
      negative:
        - "Migration effort (completed)"
        - "XML kept as reference (minor storage cost)"

  ADR005_json_schema_2020:
    status: "accepted"
    date: "2024-12-07"
    title: "JSON Schema Draft 2020-12"
    context: |
      Need schema validation for game data. Multiple JSON Schema
      versions exist.
    decision: "Use Draft 2020-12, the latest stable version"
    rationale:
      - "Best $ref resolution"
      - "Dynamic references support"
      - "Active development"
      - "referencing library support"
    consequences:
      positive:
        - "Modern features available"
        - "Good library support"
      negative:
        - "Some older tools may not support"

  ADR006_pydantic_models:
    status: "accepted"
    date: "2024-12"
    title: "Pydantic for all game objects"
    context: |
      Need runtime validation and type safety for game entities.
    decision: "All game objects are Pydantic BaseModel subclasses"
    rationale:
      - "Runtime validation"
      - "Automatic serialization"
      - "IDE support"
      - "Clear schema definition"
    consequences:
      positive:
        - "Type safety"
        - "Self-documenting models"
      negative:
        - "Slight performance overhead"
        - "Learning curve"

  ADR007_data_driven_design:
    status: "accepted"
    date: "2024-12"
    title: "Game logic in data, not code"
    context: |
      Game has many entities with varying behaviors. Hardcoding
      each case leads to unmaintainable code.
    decision: |
      Define game rules in data files (JSON/TOML).
      Code provides engine, data provides content.
    rationale:
      - "Easy content modification"
      - "Modding support"
      - "Clear separation of concerns"
      - "Non-programmers can adjust balance"
    consequences:
      positive:
        - "Flexible content"
        - "Moddable"
      negative:
        - "More complex data structures"
        - "Data validation critical"

  ADR008_test_separation:
    status: "accepted"
    date: "2024-12"
    title: "Separate fast tests from AI tests"
    context: |
      AI tests are slow and non-deterministic.
      Logic tests should be fast and reliable.
    decision: |
      Use pytest markers to separate:
      - math, ledger, topology: fast, deterministic
      - ai: slow, can be flaky
    rationale:
      - "Fast feedback loop for logic changes"
      - "CI can run fast tests on every commit"
      - "AI tests run less frequently"
    consequences:
      positive:
        - "Fast TDD cycle"
        - "Clear test categories"
      negative:
        - "Must remember to run both"

  ADR009_networkx_topology:
    status: "accepted"
    date: "2024-12-07"
    title: "NetworkX for Topology layer graph storage"
    context: |
      The Embedded Trinity architecture (ADR001) specifies a Topology layer for
      fluid relational state: class solidarity networks, dialectical relationships,
      supply chains, tension flows. This requires a graph data structure.

      Options evaluated:
      - NetworkX: Pure Python, rich algorithms, in-memory
      - igraph: C-based, faster, less Pythonic API
      - graph-tool: C++ core, fastest, complex installation
      - Neo4j: Full graph DB, requires external server
      - SQLite edge tables: DIY graph, no built-in algorithms
      - RDFLib: Semantic web focus, overkill for game state
    decision: |
      Use NetworkX as the graph engine for the Topology layer.
      Graph state persists to SQLite (Ledger) for save/load.
      At startup, build NetworkX graph from SQLite edge data.
      During gameplay, mutate graph in-memory.
      On save, serialize graph back to SQLite.
    rationale:
      - "Pure Python: no external servers, matches 'no external dependencies' principle"
      - "Rich algorithm library: centrality, paths, community detection built-in"
      - "Sufficient performance: game will have ~1000-10000 nodes max, not millions"
      - "Pythonic API: easy to test, integrates well with Pydantic models"
      - "Serializable: nx.node_link_data() exports to JSON for SQLite storage"
      - "Phase 1 scope: 2 nodes, 1 edge - any graph library works, pick simplest"
      - "TDD-friendly: graph operations are pure functions, easy to unit test"
    alternatives_rejected:
      igraph:
        reason: "C dependency complicates installation; performance gain unnecessary at our scale"
      graph_tool:
        reason: "Difficult to install (needs C++ compilation); overkill for game-sized graphs"
      neo4j:
        reason: "Violates 'no external servers' principle; adds operational complexity"
      sqlite_edges:
        reason: "Would require implementing graph algorithms from scratch; NetworkX provides them"
    consequences:
      positive:
        - "Zero external dependencies beyond Python package"
        - "Rich built-in algorithms for atomization metrics, hub detection, path analysis"
        - "Easy serialization to SQLite for game saves"
        - "Well-documented, mature library with active maintenance"
        - "Trivial to test graph logic in isolation"
      negative:
        - "Pure Python slower than C-based alternatives (acceptable at our scale)"
        - "In-memory only: requires explicit save/load logic to persist state"
        - "No concurrent write safety (not needed for single-player turn-based game)"
    performance_notes: |
      NetworkX handles 10,000+ nodes efficiently for in-memory operations.
      Phase 4 projection: 17 entity types × ~100 instances = ~1,700 nodes.
      This is well within NetworkX's comfortable range.
      If future requirements exceed 100,000 nodes, reconsider igraph migration.
    integration_pattern: |
      SQLite (Ledger)         NetworkX (Topology)        ChromaDB (Archive)
        ┌──────────┐           ┌───────────────┐           ┌──────────┐
        │ entities │──startup──▶│ G = nx.DiGraph│◀──query───│ semantic │
        │ edges    │           │ nodes + edges │           │ embeddings│
        └──────────┘           └───────────────┘           └──────────┘
             ▲                        │
             │                        │
             └───────save game────────┘

  ADR011_pure_graph_architecture:
    status: "accepted"
    date: "2024-12-07"
    title: "Pure Graph Architecture: Graph + Math = History"
    context: |
      Following ADR010 (bypass Economy/Politics), a deeper architectural review
      was conducted with multi-AI consensus (Claude + Gemini + User).

      Key insight from Gemini:
      "The previous architecture was trying to simulate INSTITUTIONS.
       The new architecture simulates MATERIAL RELATIONS.
       This is the shift from Liberalism to Materialism."

      The Economy class modeled "the economy" as a liberal abstraction.
      In MLM-TW, "the economy" doesn't exist as a thing - it's the name we give
      to the totality of production relations. The graph makes this explicit.

    decisions:
      delete_legacy_classes:
        action: "Delete Economy and Politics classes immediately"
        rationale: |
          - Not "preserve for future" - that's hoarding technical debt
          - If patterns needed later, write fresh with correct architecture
          - git log is the museum for old code
        files_to_delete:
          - "src/babylon/core/economy.py"
          - "src/babylon/core/politics.py"

      pure_graph_option_c:
        action: "Everything is nodes and edges (Option C)"
        rationale: |
          - Geography is just another relation type
          - Hierarchy exists as CONTAINS edges, doesn't constrain other edges
          - A neighborhood in Global South can have direct edge to hedge fund in NYC
          - Imperialism doesn't respect administrative boundaries - neither should our graph
        edge_types:
          - "CONTAINS (geographic hierarchy)"
          - "EXTRACTS_FROM (economic exploitation)"
          - "REPRESSES (political violence)"
          - "PRODUCES (labor value)"
          - "ORGANIZES (class solidarity)"
        example: |
          USA --[CONTAINS]--> Kansas_City
          Proletariat_KC --[EXTRACTS_FROM]--> Bourgeoisie_NY
          Police_KC --[REPRESSES]--> Proletariat_KC

      hybrid_state_management:
        action: "Snapshots for Engine, Events for Archive"
        rationale: |
          - Engine needs current state to calculate next state (Snapshots)
          - AI narrative needs history of what happened (Events)
          - Math and Story have different data needs - don't confuse them
          - Testing is cleaner: Engine tests are pure functions on snapshots
        structure:
          engine: "Immutable WorldState snapshots"
          archive: "Event log for narrative generation"

      networkx_from_day_one:
        action: "Use NetworkX immediately, not deferred"
        rationale: |
          - If concept is "everything is a graph," implementation should be a graph
          - Using dict for 2 nodes then switching later is chosen technical debt
          - NetworkX is lightweight - no cost for small graphs
          - Built-in algorithms available when needed (centrality, paths)
          - Keeps implementation honest to architecture
        revision: "Supersedes ADR010 recommendation to defer NetworkX"

      simulation_config_is_core:
        action: "SimulationConfig is part of irreducible kernel"
        rationale: |
          - Formulas are parameterized (α, λ, S, k)
          - Without coefficients, formulas are abstract math, not simulation
          - Config is as essential as State and Engine

    the_true_kernel:
      description: "The irreducible core of the simulation"
      components:
        world_state:
          purpose: "The Data"
          contains:
            - "Graph (Nodes: entities)"
            - "Graph (Edges: relations)"
            - "tick: current turn"

        simulation_config:
          purpose: "The Constants"
          contains:
            - "α (extraction_efficiency)"
            - "λ (loss_aversion = 2.25)"
            - "S (subsistence_threshold)"
            - "k (consciousness_sensitivity)"
            - "repression_level"

        formula_library:
          purpose: "The Laws"
          pattern: "f(data, config) → value"
          location: "babylon.systems.formulas"

        simulation_engine:
          purpose: "The Time"
          pattern: "step(state, config) → new_state"

      equation: "Graph + Math = History"

    philosophical_grounding:
      insight: |
        "The Economy is not a class; it is the sum of all EXTRACTS_FROM edges."
        "Politics is not a class; it is the sum of all REPRESSES edges."

        In materialist analysis, abstractions like "the economy" or "politics"
        don't exist as things. They are names for the totality of material relations.
        The graph architecture makes this explicit:
        - No Economy node exists
        - Only relations of extraction, production, exchange
        - "The economy" is a QUERY over those edges, not an entity

      alignment: "Architecture encodes theory. The code IS the analysis."

    consequences:
      positive:
        - "Code matches MLM-TW theoretical framework"
        - "No liberal abstractions hiding material relations"
        - "Flexible: any node can relate to any other node"
        - "Queryable: 'total extraction' is sum of EXTRACTS_FROM edges"
        - "Testable: graph operations are well-understood"
        - "Scalable: NetworkX handles large graphs efficiently"
      negative:
        - "Requires deleting existing code (minimal loss)"
        - "Slightly more abstract than class-based approach"
        - "Team must understand graph thinking"

    consensus:
      participants:
        - "Claude (Opus 4.5)"
        - "Gemini (via user)"
        - "User (Persephone)"
      mantra: "Graph + Math = History"
      supersedes:
        - "ADR010 NetworkX deferral recommendation"

  ADR010_direct_formula_architecture:
    status: "accepted"
    date: "2024-12-07"
    title: "Direct Entities + Formulas architecture (bypass Economy/Politics classes)"
    context: |
      During Phase 2 design review, discovered THREE disconnected math systems:

      1. formulas.py (349 lines, 40 tests)
         - MLM-TW formulas: imperial rent, survival calculus, consciousness drift
         - Pure functions, theoretically grounded, TESTED

      2. Economy class (core/economy.py, 148 lines, 0 tests)
         - Generic economic simulation with its own internal formulas
         - Has update() method but doesn't use formulas.py
         - UNTESTED

      3. Politics class (core/politics.py, 164 lines, 0 tests)
         - Generic political simulation with its own internal formulas
         - Has update() method but doesn't use formulas.py
         - UNTESTED

      4. ContradictionAnalysis (systems/contradiction_analysis.py, 20 tests)
         - Tension tracking and phase transitions
         - Works well but doesn't call formulas.py either

      These systems are completely disconnected. Economy._update_class_relations()
      uses generic formulas, not the MLM-TW formulas we carefully designed and tested.

      Options evaluated:
      A. Replace Economy/Politics entirely with new subsystems
      B. Refactor Economy/Politics to use formulas.py internally
      C. Direct Entities + Formulas (no subsystems)
    decision: |
      Option C: Direct Entities + Formulas architecture for Phase 2.

      SimulationEngine.step() calls formulas directly on entities:

      1. For each Relationship:
         - Call calculate_imperial_rent()
         - Update entity wealth

      2. For each SocialClass:
         - Call calculate_consciousness_drift()
         - Call calculate_acquiescence_probability()
         - Call calculate_revolution_probability()
         - Update entity state

      3. For each Contradiction:
         - Update tension based on entity states
         - Check for rupture/synthesis via ContradictionAnalysis

      No Economy class. No Politics class in the game loop.
      Just: Entities (data) + Formulas (math) + Engine (orchestration)
    rationale:
      - "Simplest possible architecture for 2-node Phase 2"
      - "formulas.py has 40 tests; Economy/Politics have 0 tests"
      - "Avoids two layers of abstraction"
      - "MLM-TW formulas ARE the theory; generic subsystems obscure it"
      - "Single source of truth for calculations"
      - "Easier to trace: entity field -> formula -> new value"
      - "YAGNI: Economy/Politics may be useful later, but not needed now"
    alternatives_rejected:
      replace_economy_politics:
        reason: "Throws away existing code; may be useful for future complexity"
      refactor_to_use_formulas:
        reason: "Adds abstraction layer without benefit for 2-node simulation"
    consequences:
      positive:
        - "Direct traceability from entity to formula to result"
        - "No untested code in critical path"
        - "Simpler debugging: fewer layers"
        - "Engine logic matches theoretical documentation"
      negative:
        - "Economy/Politics classes deleted (see ADR011)"
        - "May need to introduce subsystems when scaling to many entities"
        - "Engine.step() may grow complex (mitigate with clear sections)"
    related_gap_analysis:
      gap_1: "Entity-to-Aggregate: Aggregates computed from entities, not stored"
      gap_2: "Formula-to-Entity wiring: Explicit mapping required (see game-loop-architecture.yaml)"
      gap_3: "This decision resolves the disconnected systems gap"
      gap_4: "Initialization: Factory function for test scenarios"
      gap_5: "Phase transitions: ContradictionAnalysis handles rupture/synthesis"
      gap_6: "Coefficients: SimulationConfig object holds global constants"
      gap_7: "NetworkX: Used from day one per ADR011 (supersedes original ADR010 recommendation)"
      gap_8: "Testing: Property-based tests for feedback loop verification"
    future_considerations: |
      When scaling beyond Phase 2 (many entities, multiple countries),
      may introduce subsystem layer for:
      - Aggregate calculations (GDP computed from all entities)
      - Batch updates (process all entities of a type together)
      - Performance optimization (cache intermediate results)

      Note: Economy/Politics classes were DELETED per ADR011.
      If subsystems are needed in future, write fresh with correct architecture.
      git log preserves the old code for reference if needed.

    implementation_status: |
      IMPLEMENTED in Phase 2 with 704 tests proving:
      - Direct formula wiring works
      - Feedback loops produce emergent behavior
      - No subsystem layer needed for 2-node simulation

  ADR012_service_container:
    status: "accepted"
    date: "2025-12-08"
    title: "ServiceContainer for Dependency Injection"
    context: |
      Phase 2 introduced modular Systems (ImperialRentSystem, ConsciousnessSystem,
      SurvivalSystem, ContradictionSystem). Each System needs access to:
      - SimulationConfig (formula coefficients)
      - FormulaRegistry (hot-swappable formulas)
      - EventBus (publish events like rupture)
      - DatabaseConnection (persistence)

      Passing these as individual parameters would require 4+ parameters per
      System.step() call, making signatures unwieldy and changes difficult.

    decision: |
      Create ServiceContainer dataclass that aggregates all services.
      Systems receive a single ServiceContainer instance instead of
      multiple individual dependencies.

      @dataclass
      class ServiceContainer:
          config: SimulationConfig
          database: DatabaseConnection
          event_bus: EventBus
          formulas: FormulaRegistry

    rationale:
      - "Single parameter simplifies System.step() signatures"
      - "Adding new services doesn't change existing signatures"
      - "Container is frozen/immutable - thread-safe if needed later"
      - "Easy to mock for testing - replace container with test version"
      - "Follows Dependency Injection pattern"

    location: "src/babylon/engine/services.py"

    consequences:
      positive:
        - "Clean System signatures: step(graph, services, context)"
        - "Easy to extend with new services"
        - "Testable via mock containers"
      negative:
        - "One more abstraction layer"
        - "Services accessed via container.config vs directly"

  ADR013_system_protocol:
    status: "accepted"
    date: "2025-12-08"
    title: "System Protocol for Modular Engine Architecture"
    context: |
      ADR010/ADR011 established Direct Entities + Formulas architecture.
      As the engine grew, step() function became monolithic with
      multiple concerns: rent extraction, consciousness drift, survival
      probabilities, tension accumulation.

      Need modular approach where each concern is isolated but can be
      composed into a single game loop.

    decision: |
      Define System protocol with single method:
        step(graph: nx.DiGraph, services: ServiceContainer, context: dict) -> None

      Each System mutates the graph in place following "historical materialist order":
      1. ImperialRentSystem - Economic base (extraction, wealth transfer)
      2. ConsciousnessSystem - Ideological superstructure (consciousness drift)
      3. SurvivalSystem - Political calculation (P(S|A), P(S|R))
      4. ContradictionSystem - Dialectical evolution (tension, rupture)

      SimulationEngine orchestrates Systems via run_tick().

    rationale:
      - "Single Responsibility: each System handles one concern"
      - "Open/Closed: add new Systems without modifying Engine"
      - "Testable: each System can be unit tested in isolation"
      - "Order encodes theory: base before superstructure"
      - "Flexible composition: can run subsets for testing"

    location: "src/babylon/engine/systems/protocol.py"

    systems_implemented:
      - name: "ImperialRentSystem"
        file: "economic.py"
        purpose: "Extract imperial rent from exploitation edges"
      - name: "ConsciousnessSystem"
        file: "ideology.py"
        purpose: "Apply consciousness drift based on material conditions"
      - name: "SurvivalSystem"
        file: "survival.py"
        purpose: "Update P(S|A) and P(S|R) survival probabilities"
      - name: "ContradictionSystem"
        file: "contradiction.py"
        purpose: "Accumulate tension, publish rupture events"

    consequences:
      positive:
        - "Clean separation of concerns"
        - "Easy to add new Systems (e.g., RepresentationSystem for Phase 3)"
        - "Each System independently testable"
        - "Clear execution order documents historical materialist order"
      negative:
        - "Graph mutations must be coordinated (Systems see each other's changes)"
        - "More files to navigate"

    test_count_impact: "Added 17+ tests for System implementations (704 total)"

  ADR014_vertical_slice_scope:
    status: "accepted"
    date: "2025-12-08"
    title: "Vertical Slice Scope for Phase 3 Completion"
    context: |
      Phase 3 aims to add AI narrative generation to the simulation. Before
      horizontal expansion (more entities, locations, phenomena), we need a
      complete vertical slice proving the full data flow works:

      Simulation → Events → RAG Context → LLM Prompt → Narrative Output

      The question is: what is the minimum scope for this vertical slice?

    decision: |
      Vertical slice scope:
      - Entities: Factory worker (proletariat) vs Capitalist (bourgeoisie) - 2 nodes
      - Location: One city (implicit, not modeled as separate entity)
      - Phenomenon: Wealth transfer via Imperial Rent - single economic mechanism
      - LLM: DeepSeek API (OpenAI-compatible, cost-effective)
      - RAG: MVP corpus from Marxists.org (5 curated texts)
      - Verification: Integration tests (not CLI runner)

      Explicitly deferred:
      - Full RAG corpus ingestion (MVP uses 5 texts only)
      - NiceGUI frontend (Phase 4)
      - Multiple scenarios/entity types
      - Save/load game state with narrative
      - Multiple cities or geographic hierarchy
      - Alternative LLM providers (Ollama, Claude)

    rationale:
      - "Proves complete data flow with minimal complexity"
      - "Two-node scenario already proven in Phase 2 integration tests"
      - "Imperial Rent is the core MLM-TW mechanic - wealth extraction"
      - "DeepSeek is cost-effective and OpenAI-compatible (easy integration)"
      - "Marxists.org corpus provides authentic theoretical grounding"
      - "Integration tests enable CI/CD and programmatic verification"
      - "Deferred features can be added after vertical slice validates approach"

    implementation:
      sprint_3_3_deliverables:
        - "LLM Provider protocol + DeepSeek implementation"
        - "Event generation in ImperialRentSystem (exploitation events)"
        - "RAG corpus ingestion script (5 texts from Marxists.org)"
        - "Integration tests proving full narrative pipeline"
      estimated_code: "~400-500 lines"
      estimated_tests: "~20 tests"
      rag_corpus:
        source: "Marxists.org archive"
        texts:
          - "Wage Labour and Capital (Marx, 1847)"
          - "Value, Price and Profit (Marx, 1865)"
          - "Principles of Communism (Engels, 1847)"
          - "Imperialism, the Highest Stage of Capitalism Ch.1-4 (Lenin, 1917)"
          - "The Wretched of the Earth Ch.1 (Fanon, 1961)"
        estimated_chunks: "200-500"

    gaps_addressed:
      gap_1: "No LLM client → DeepSeek Provider implementation"
      gap_2: "No events until rupture → ImperialRentSystem event generation"
      gap_3: "Empty RAG corpus → 5 texts from Marxists.org"
      gap_4: "No verification → Integration tests with assertions"

    consequences:
      positive:
        - "Clear, achievable scope for Sprint 3.3"
        - "Validates architecture before horizontal expansion"
        - "Real theoretical content in RAG (not just static quotes)"
        - "Programmatic testing enables CI/CD"
        - "Deferred features remain possible without rework"
      negative:
        - "Not a 'complete game' - just proves the tech stack"
        - "DeepSeek API requires internet and API key"
        - "Limited corpus may not cover all narrative scenarios"

    success_criteria: |
      Run integration tests, verify:
      1. Simulation advances (tick increases)
      2. Wealth transfers (worker impoverished)
      3. Exploitation events generated and logged
      4. RAG retrieves relevant Marxist theory
      5. DeepSeek API called with proper context
      6. Narrative text generated and captured
      7. All assertions pass in pytest

  ADR015_llm_provider_abstraction:
    status: "accepted"
    date: "2025-12-08"
    title: "LLM Provider Protocol with DeepSeek as Primary"
    context: |
      The NarrativeDirector needs to call an LLM to generate narrative. Multiple
      LLM options exist with different tradeoffs:
      - DeepSeek: Cost-effective, OpenAI-compatible API, good quality
      - Ollama: Local, free, variable quality, no API key needed
      - Claude (Anthropic): Highest quality, higher API costs

      Need abstraction to support all without coupling Director to specific provider.

    decision: |
      Create LLMProvider protocol with DeepSeek as primary implementation:

      @runtime_checkable
      class LLMProvider(Protocol):
          def generate(self, prompt: str, system: str | None = None) -> str:
              '''Generate text from prompt.'''
              ...

      Primary implementation (Sprint 3.3):
      - DeepSeekProvider (OpenAI-compatible API)

      Future implementations (as needed):
      - OllamaProvider (HTTP API)
      - ClaudeProvider (Anthropic SDK)

      NarrativeDirector accepts optional LLMProvider in constructor.
      If None, narrative is logged but not generated (backward compat).

    rationale:
      - "DeepSeek is cost-effective for development/testing"
      - "OpenAI-compatible API means we can use openai Python package"
      - "Protocol pattern matches existing SimulationObserver design"
      - "Optional injection maintains backward compatibility with tests"
      - "Easy to add new providers without modifying Director"
      - "Sync API matches Observer pattern (not async)"

    location: "src/babylon/ai/llm_provider.py"

    deepseek_integration:
      api_base: "https://api.deepseek.com"
      model: "deepseek-chat"
      package: "openai (with custom base_url)"
      env_var: "DEEPSEEK_API_KEY"

    consequences:
      positive:
        - "Cost-effective development and testing"
        - "Flexible provider selection for future"
        - "Easy testing with mock providers"
        - "No vendor lock-in"
        - "OpenAI SDK provides robust client implementation"
      negative:
        - "Requires API key and internet connection"
        - "One more abstraction layer"
        - "Rate limits and API costs (though minimal with DeepSeek)"

  ADR016_fascist_bifurcation:
    status: "accepted"
    date: "2025-12-09"
    title: "The Fascist Bifurcation: Solidarity as Infrastructure"
    context: |
      Sprint 3.4.2 implemented Proletarian Internationalism, but a critical design
      question emerged: should solidarity_strength be auto-calculated from source
      organization, or stored as a persistent edge attribute?

      Historical observation: Accelerationist strategies (crash the economy to
      trigger revolution) consistently produce fascism, not socialism. Germany 1933,
      Italy 1922, Spain 1936. Why?

      Answer: Material disruption (wage decline) creates "agitation energy" that
      has NO INHERENT DIRECTION. It can flow toward class consciousness OR toward
      national chauvinism/scapegoating. The direction depends on PRE-EXISTING
      solidarity infrastructure (unions, internationals, worker organizations).

      If solidarity infrastructure exists: agitation → class awakening → revolution
      If solidarity infrastructure absent: agitation → fascist turn → reaction

      This is the core insight of anti-accelerationism in MLM-TW theory.

    decision: |
      Store solidarity_strength as a PERSISTENT EDGE ATTRIBUTE, not auto-calculated.

      Field: Relationship.solidarity_strength: Coefficient = Field(default=0.0)

      Key semantics:
      - solidarity_strength = 0.0 means NO solidarity infrastructure
      - Must be BUILT through player/system actions (like the 3rd International)
      - High organization without solidarity infrastructure = Fascist Bifurcation risk

      Formula: dΨ_target = σ_edge × (Ψ_source - Ψ_target)
      - σ_edge is READ FROM EDGE DATA, not calculated from source
      - If σ_edge = 0: no transmission even if source is revolutionary
      - This creates the Fascist Bifurcation branch point

    rationale:
      - "Encodes anti-accelerationist theory into mechanics"
      - "Enables emergent Fascist vs Revolutionary outcomes from same starting conditions"
      - "Player agency: must choose to BUILD solidarity before crisis"
      - "Historical accuracy: 3rd International had to be ORGANIZED"
      - "Same code produces both outcomes based on prior state"

    implementation:
      location: "src/babylon/models/entities/relationship.py"
      formula: "src/babylon/systems/formulas.py:calculate_solidarity_transmission()"
      system: "src/babylon/engine/systems/solidarity.py"
      tests: "tests/unit/engine/systems/test_solidarity_system.py"

    scenarios:
      revolutionary_outcome: |
        Setup: P_w (Ψ=0.9), C_w (Ψ=0.1), SOLIDARITY edge (σ=0.8)
        Transmission: delta = 0.8 × (0.9 - 0.1) = 0.64
        Result: C_w awakens, revolutionary consciousness spreads
        Historical analogue: International solidarity during 1917-1919

      fascist_outcome: |
        Setup: P_w (Ψ=0.9), C_w (Ψ=0.1), SOLIDARITY edge (σ=0.0)
        Transmission: delta = 0.0 × (0.9 - 0.1) = 0.0
        Result: C_w loses wages but has no class explanation
        Scapegoating fills the void → Fascist turn
        Historical analogue: Germany 1929-1933

    consequences:
      positive:
        - "Emergent political outcomes from same economic crisis"
        - "Player must invest in solidarity BEFORE crisis"
        - "Encodes 'accelerationism produces fascism' as game mechanic"
        - "Historically accurate: internationals were BUILT, not automatic"
      negative:
        - "More edge state to manage"
        - "Requires UI to show solidarity infrastructure status"

    mantra: "Agitation without solidarity produces fascism, not revolution."

  ADR017_fractal_topology:
    status: "proposed"
    date: "2025-12-09"
    title: "Fractal Topology for Internal Colonies"
    context: |
      The 4-node Imperial Circuit (P_w, P_c, C_b, C_w) models Core-Periphery
      dynamics but treats the Core as monolithic. In reality, the Core contains
      "Internal Colonies" - stratified populations with different relationships
      to imperial rent.

      Examples:
      - Black Americans: super-exploitation within Core, lower access to imperial rent
      - Women workers: gendered wage gap, unpaid reproductive labor
      - Rural workers: geographic stratification, deindustrialized regions
      - Immigrant workers: legal precarity, wage depression

      These groups experience "Periphery-like" conditions within the Core.
      The graph needs to model this without code explosion.

    decision: |
      Use Composite/Graph pattern for fractal topology:
      1. C_w node can be EXPANDED into sub-graph when needed
      2. Sub-nodes (C_w_black, C_w_white, C_w_immigrant, etc.) inherit from parent
      3. Edges can cross fractal levels (C_b → C_w_black)
      4. Aggregation: collapsed C_w = weighted sum of sub-nodes

      Pattern:
      - Phase 3.4: Single C_w node (current)
      - Phase 5: Expand C_w into Internal Colony sub-graph
      - Same engine code works at any fractal level

    rationale:
      - "Models real stratification without monolithic code changes"
      - "Enables 'zoom in' mechanics for UI"
      - "Same formulas apply at every fractal level"
      - "Captures 'internal colonization' of oppressed groups"
      - "Graph-native: NetworkX supports subgraphs naturally"

    implementation_sketch:
      pattern: "Composite Design Pattern + NetworkX subgraph"
      expansion: |
        # Phase 5 implementation
        C_w_expanded = {
          "C_w_white": {..., parent: "C_w", weight: 0.6},
          "C_w_black": {..., parent: "C_w", weight: 0.13},
          "C_w_latino": {..., parent: "C_w", weight: 0.19},
          "C_w_immigrant": {..., parent: "C_w", weight: 0.08},
        }
      aggregation: |
        C_w_aggregate = Σ(sub_node.wealth × sub_node.weight)

    blockers:
      - "Phase 3.4 Multi-Dimensional Consciousness must be complete"
      - "Phase 4 UI needed to visualize fractal zoom"

    consequences:
      positive:
        - "Models Internal Colonies without special-casing"
        - "Same engine handles any fractal depth"
        - "Enables future complexity scaling"
        - "Graph is already the right data structure"
      negative:
        - "Aggregation complexity increases"
        - "UI must handle variable graph depth"
        - "Testing surface grows with fractal levels"

    theoretical_basis:
      sources:
        - "Settlers: The Mythology of the White Proletariat (J. Sakai)"
        - "Black Reconstruction in America (W.E.B. Du Bois)"
        - "Caliban and the Witch (Silvia Federici)"
      insight: |
        The white working class in America has historically been granted
        "psychological wage" (Du Bois) - a sense of superiority that
        compensates for economic exploitation. This is the "internal
        stratification" that fractal topology must model.

  ADR018_logging_strategy:
    status: "accepted"
    date: "2025-12-09"
    title: "Python stdlib logging with custom enhancements"
    context: |
      Babylon needs a logging strategy that supports:
      - Developer debugging (human-readable console)
      - Post-mortem analysis (machine-parseable files)
      - Exception integration (structured error context)
      - Performance awareness (conditional logging for hot paths)
      - Layer-specific concerns (Ledger, Topology, Archive, Observer)

      Options evaluated:
      - Python stdlib logging: Zero deps, universal knowledge, full control
      - loguru: Beautiful output, zero config, but adds dependency
      - structlog: Best structured logging, but overkill for game

      Current state had several issues:
      - logging.yaml references pythonjsonlogger (not in dependencies)
      - __main__.py has basicConfig conflicting with setup_logging()
      - context_window/manager.py logs every add/remove (performance)
      - No log rotation, correlation IDs, or TRACE level

    decision: |
      Stay with Python stdlib logging, add custom enhancements:

      1. Custom JSONFormatter (no dependencies)
         - JSON Lines format for file output
         - ~30 lines of code, no pythonjsonlogger needed

      2. Custom TRACE level (value=5)
         - Ultra-verbose debugging below DEBUG
         - Never enabled in production

      3. Rotating file handlers
         - babylon.log: 10MB x 5 = 50MB max
         - errors.log: ERROR+ only, 5MB x 10 = 50MB

      4. Exception integration
         - BabylonError.log() helper method
         - Log at handling boundary, not raise site
         - Use exception.to_dict() for structured context

      5. Per-simulation logs (future)
         - sim_{timestamp}_{seed}.jsonl for replay/analysis

    rationale:
      - "User preference: minimal dependencies"
      - "User preference: loves custom loggers"
      - "Exception hierarchy already has to_dict() - leverage it"
      - "stdlib is universal - any Python dev can work with it"
      - "Self-contained project - no library interop concerns"
      - "Custom enhancements are simple (~100 lines total)"

    log_levels:
      CRITICAL: "System unusable, data loss imminent"
      ERROR: "Operation failed, system continues"
      WARNING: "Unexpected but handled"
      INFO: "Normal operation milestones"
      DEBUG: "Detailed diagnostics"
      TRACE: "Ultra-verbose (custom, value=5)"

    output_formats:
      console:
        format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
        level: "INFO (default)"
      file:
        format: "JSON Lines"
        level: "DEBUG"
        rotation: "10MB x 5 backups"

    exception_logging_principle: |
      "Log at the handling boundary"
      - Don't log at raise site (causes duplicates on re-raise)
      - Log at final handler (where exception is resolved)
      - Use exception.to_dict() for structured context
      - Caller decides importance (retry? fallback? fatal?)

    implementation:
      files_to_create:
        - "src/babylon/utils/log.py (JSONFormatter, TRACE, helpers)"
      files_to_modify:
        - "src/babylon/config/logging_config.py (rotation, handlers)"
        - "src/babylon/__main__.py (remove basicConfig)"
        - "src/babylon/utils/exceptions.py (add log() method)"
        - "logging.yaml (fix pythonjsonlogger reference)"
        - "src/babylon/rag/context_window/manager.py (add guards)"

    detailed_spec: "ai-docs/logging-architecture.yaml"

    consequences:
      positive:
        - "Zero new dependencies"
        - "Custom formatters exactly fit our needs"
        - "Exception error_code in all log entries"
        - "Machine-parseable JSONL for analysis"
        - "Rotation prevents disk bloat"
        - "TRACE available for deep debugging"
      negative:
        - "Must maintain custom code (~100 lines)"
        - "No fancy log aggregation features out of box"
        - "Correlation ID propagation is manual"

    alternatives_rejected:
      loguru:
        reason: "Adds dependency; different API requires learning curve"
      structlog:
        reason: "Overkill complexity for game project; steep learning curve"
      pythonjsonlogger:
        reason: "Can write equivalent in 30 lines; avoids dependency"

  ADR019_exception_hierarchy:
    status: "accepted"
    date: "2025-12-09"
    title: "Unified exception hierarchy (33 → 10 classes)"
    context: |
      Original codebase had 33 exception classes across multiple modules with:
      - Unclear inheritance relationships
      - Inconsistent error code usage
      - No structured serialization
      - Proliferating special-case exceptions

      Needed: Clear hierarchy aligned with architectural layers (Embedded Trinity)
      that supports machine-parseable error codes and structured logging.

    decision: |
      Consolidate to 10 core exception classes using Mikado Method refactoring:

      BabylonError (base)
      ├── InfrastructureError (retryable I/O)
      │   ├── DatabaseError
      │   └── StorageError → CheckpointIOError → 3 children
      ├── ValidationError (bad input)
      │   └── ConfigurationError
      ├── SimulationError (fatal engine)
      │   └── TopologyError
      └── ObserverError (non-fatal AI/RAG)
          ├── LLMError
          └── RagError → 15 aliases for backwards compatibility

      All exceptions have:
      - message: Human-readable description
      - error_code: PREFIX_NNN format (e.g., LLM_002, RAG_401)
      - details: dict for structured context
      - to_dict(): JSON-serializable representation

    rationale:
      - "Layer alignment: Exceptions match Embedded Trinity architecture"
      - "Semantic codes: PREFIX_NNN enables machine parsing and filtering"
      - "Catch hierarchy: 'except ObserverError' catches all AI/RAG errors"
      - "Non-fatal observers: AI failures never crash simulation (ADR003)"
      - "Backwards compatibility: 15 aliases preserve old exception names"
      - "Structured logging: to_dict() integrates with JSONFormatter"

    error_code_prefixes:
      SYS: "System-level / base errors"
      INFRA: "Infrastructure errors"
      STOR: "Storage/file errors"
      DB: "Database errors"
      VAL: "Validation errors"
      CFG: "Configuration errors"
      SIM: "Simulation errors"
      TOP: "Topology/graph errors"
      OBS: "Observer layer errors"
      LLM: "LLM generation errors"
      RAG: "RAG system errors (100-599 ranges)"

    key_principle: |
      Log at the HANDLING boundary, not at the raise site.
      The caller knows if an exception is worth logging.
      Use exception.to_dict() for structured context.

    test_coverage:
      file: "tests/unit/utils/test_exceptions.py"
      count: 93
      categories:
        - "Hierarchy inheritance"
        - "Default error codes"
        - "Custom error codes"
        - "Constructor behavior"
        - "Serialization (to_dict, __str__, __repr__)"
        - "Backwards compatibility aliases"
        - "Exception propagation"
        - "Import path consistency"
        - "Edge cases (pickle, unicode, chaining)"

    files_modified:
      - "src/babylon/utils/exceptions.py (core hierarchy)"
      - "src/babylon/exceptions.py (re-exports)"
      - "src/babylon/rag/exceptions.py (RagError + aliases)"
      - "src/babylon/rag/context_window/__init__.py (aliases)"
      - "src/babylon/engine/history/io.py (checkpoint exceptions)"

    consequences:
      positive:
        - "Clear 4-layer hierarchy matches architecture"
        - "Machine-parseable error codes for log analysis"
        - "Structured to_dict() for JSON logging"
        - "Single catch point per layer (e.g., ObserverError)"
        - "93 tests lock down behavior"
        - "Backwards compatible via aliases"
      negative:
        - "Must remember to use appropriate error_code"
        - "Aliases may confuse new developers"
        - "Some module-specific exceptions still exist (CheckpointIOError)"

    detailed_spec: "ai-docs/exceptions-architecture.yaml"

  ADR020_parameter_tuning_methodology:
    status: "accepted"
    date: "2025-12-11"
    title: "Sensitivity Analysis for GameDefines Tuning"
    context: |
      GameDefines contains all simulation coefficients externalized via the Paradox Refactor.
      However, initial values were educated guesses, not empirically validated.

      Problem discovered: extraction_efficiency = 0.8 (default) kills the periphery in
      ~9 ticks, making the game unplayable. No dynamics have time to develop.

      Need systematic approach to find "Playable Boundaries" - parameter values where
      the simulation is challenging but allows meaningful gameplay.

    decision: |
      Create tools/tune_parameters.py for automated sensitivity analysis:

      1. Parameter Sweep: Iterate through value ranges (start, end, step)
      2. Simulation Run: Execute 50 ticks per parameter value
      3. Death Detection: Track when periphery wealth <= 0.001
      4. Metrics Collection: ticks_survived, max_tension, outcome
      5. Boundary Analysis: Find value where survival crosses threshold

      Mise tasks:
      - mise run tune-params (default extraction_efficiency sweep)
      - mise run tune-params-custom (custom parameter sweeps)

      Findings recorded in ai-docs/balance-tuning.yaml.

    rationale:
      - "Empirical validation beats theoretical guessing"
      - "Reproducible: same sweep produces same results (deterministic sim)"
      - "Automated: can re-run after any formula changes"
      - "Documented: balance-tuning.yaml captures findings for future reference"
      - "Reveals design issues: Tension Inversion anomaly discovered this way"

    tool_location: "tools/tune_parameters.py"
    findings_location: "ai-docs/balance-tuning.yaml"
    mise_tasks:
      - "tune-params"
      - "tune-params-custom"

    key_findings:
      extraction_efficiency:
        default: 0.8
        playable_boundary: 0.25
        recommended: 0.20
        issue: "Default kills periphery in 9 ticks - unplayable"

      tension_inversion_anomaly:
        observation: "Higher extraction → LOWER max tension"
        cause: "Death occurs before tension accumulates"
        implication: "Current model may need tension rate adjustment"

    consequences:
      positive:
        - "Data-driven balance decisions"
        - "Reproducible methodology"
        - "Catches unplayable configurations before release"
        - "Documents rationale for parameter values"
        - "Reveals formula design issues early"
      negative:
        - "Each sweep takes ~30 seconds (50 ticks × 10 values)"
        - "Only tests one parameter at a time (no interaction effects)"
        - "Requires manual analysis of anomalies"

    future_enhancements:
      - "Multi-parameter grid search"
      - "Automated anomaly detection"
      - "Visualization of parameter landscapes"

# =============================================================================
# PENDING DECISIONS
# =============================================================================

pending:

  PDR001_ui_framework:
    status: "pending"
    options:
      - "NiceGUI (Python-native)"
      - "Obsidian-like custom renderer"
      - "Terminal-only (Rich)"
    leaning: "NiceGUI for game, Obsidian-like for wiki"
    blockers: "Game loop needs to exist first"
    aesthetic_direction:
      name: "The Digital Grow Room"
      status: "approved"
      spec: "brainstorm/ui/digital-grow-room.md"
      summary: |
        Hydroponic Cyber-Insurgency aesthetic. You are the virus in the machine,
        the internal contradiction growing revolution in the basement of empire.
        Jury-rigged high-tech: GPUs cooling in a damp basement, purple LED grow
        lights reflecting off CRT monitors, terminal green data streams.
      palette:
        void: "#050505 (dark basement)"
        grow_light: "#9D00FF (life of revolution)"
        data: "#39FF14 (terminal green)"
        heat: "#FF3333 (rupture/danger)"
      typography:
        - "JetBrains Mono (code/data)"
        - "VCR OSD Mono (glitch headers)"
      metaphors:
        topology: "Attack Surface / Network Scanner"
        narrative: "Root Shell / Terminal stdout"
        controls: "Environmental Controls (fan speed, voltage)"
        tension: "GPU Temperature gauge"
        repression: "Heat Signature warning"
        resources: "Power Draw meter"
        ai: "The Gardener"
      widgets:
        doomsday_clock:
          system: "Time"
          concept: "Clock showing tick count, hand approaches midnight"
          visual: "Bulletin of Atomic Scientists aesthetic, glows red near 12:00"
          thematic: "The revolution is inevitable. The only question is when."
        sankey_diagram:
          system: "Imperial Rent"
          concept: "Wealth flow visualization Periphery → Core"
          visual: "Thick flowing lines, gradient color, animated particles"
          thematic: "Make the invisible visible. Show where wealth actually goes."
        tension_gauge:
          system: "Contradiction"
          concept: "Analog needle gauge, glass cracks at 1.0"
          visual: "Retro industrial gauge, green→yellow→red zones"
          thematic: "Pressure builds. The system shatters."
        political_compass:
          system: "Consciousness"
          concept: "Scatter plot of population ideological drift"
          visual: "X=Class Consciousness, Y=Internationalism, dots drift over time"
          thematic: "Watch the masses awaken. Or fall to fascism."
        dossier_modal:
          system: "Inspection"
          concept: "Click node → detailed Ledger data modal"
          visual: "Tabbed interface: Overview | Raw JSON | History | Relationships"
          thematic: "Intelligence gathering. Know your enemy. Know your comrades."

  PDR002_narrative_delivery:
    status: "brainstorm"
    title: "The Gramscian Wire - Narrative Delivery System"
    reference: "brainstorm/narrative/gramscian-wire.md"
    supersedes: "brainstorm/gramscian-wiki-engine.md (original wiki concept)"
    pivot: |
      From "standalone wiki engine" to "Victoria-style newspaper announcements"
      with Gramscian hegemony mechanics. The wiki IS the narrative delivery.
      You're not reading newspapers - you're intercepting information streams.
    concept:
      name: "The Wire"
      channels:
        corporate_feed: "Hegemonic perspective, 'neutral' bourgeois framing"
        liberated_signal: "Counter-hegemonic, revolutionary perspective"
        intel_briefing: "Player faction analysis, actionable data"
      key_insight: "Same event, different framings - ideology made visible"
    new_mechanics:
      hegemony_stat:
        description: "Faction attribute for cultural/narrative control"
        effect: "Determines which perspective is 'mainstream'"
      propaganda_actions:
        - "Underground press"
        - "Radio broadcast"
        - "Viral campaign"
      reader_effect: "What player reads affects consciousness (optional)"
    implementation_phases:
      phase_1: "Single perspective (current NarrativeDirector)"
      phase_2: "Dual perspective (Bourgeois + Revolutionary)"
      phase_3: "Full N-faction with hegemony mechanics"
    theoretical_basis: |
      Gramsci's cultural hegemony: Ruling class maintains power through
      consent, not just coercion. Media naturalizes capitalist worldview.
      Counter-hegemony provides alternative framing.
    integration:
      extends: "src/babylon/ai/director.py (NarrativeDirector)"
      ui: "brainstorm/ui/digital-grow-room.md (The Wire panel)"

  ADR021_cicd_philosophy:
    status: "accepted"
    date: "2025-12-11"
    title: "CI/CD Philosophy: Block on Correctness, Not Style"
    context: |
      Initial CI pipeline had 6+ jobs including yamllint, markdownlint, and
      Python 3.13 matrix testing. This created friction:
      - Prose in ai-docs/ failed yamllint line length checks
      - Style differences blocked merges when pre-commit wasn't run locally
      - Python 3.13 tests were informational noise (not production target)

      Question: What should CI actually block on for a game project?

    decision: |
      Simplified CI to 3 jobs with clear philosophy:

      1. **ci** (blocks merge): Lint, Type Check, Test
         - Ruff check (likely bugs)
         - MyPy strict (type errors)
         - Pytest -m "not ai" (functionality)

      2. **docs** (blocks merge): Documentation Build
         - Sphinx -W (warnings as errors)
         - Doctest modules (example code works)

      3. **style** (informational): Style Check
         - Ruff format --check
         - continue-on-error: true (never blocks)

      Extended analysis (releases/weekly) in separate workflow:
      - Python version matrix (3.12, 3.13)
      - Parameter sweep analysis
      - AI/RAG evaluation tests

    rationale:
      - "This is a game, not enterprise banking software"
      - "Style is local concern (pre-commit hooks)"
      - "CI should catch bugs, not formatting"
      - "Reduce friction = more commits = faster iteration"
      - "Extended tests run when useful, not on every PR"

    files:
      - ".github/workflows/ci.yml"
      - ".github/workflows/extended-analysis.yml"

    consequences:
      positive:
        - "Faster CI runs (~2 min vs ~5 min)"
        - "Less false-positive blocking"
        - "Clear separation of concerns"
        - "ai-docs/ can have prose-length lines"
      negative:
        - "Style issues may slip through if pre-commit skipped"
        - "Python 3.13 compatibility not continuously tested"

    related_mantras:
      - "block_on_correctness"
      - "test_logic_not_ai"

  ADR022_sphinx_autosummary_config:
    status: "accepted"
    date: "2025-12-11"
    title: "Configure Sphinx to Handle Python Re-Exports"
    context: |
      Sphinx documentation build produced 2016 warnings, primarily:
      "duplicate object description of babylon.engine.Event"

      Investigation revealed the root cause:

      Python's __init__.py re-exports classes for convenient imports:
      ```python
      # babylon/engine/__init__.py
      from babylon.engine.event_bus import Event
      __all__ = ["Event", ...]
      ```

      This is STANDARD Python API design (used by pandas, requests, pydantic).
      Users import `from babylon.engine import Event` instead of the full path.

      Sphinx's autosummary with `autosummary_generate = True` documents
      classes in BOTH locations:
      1. babylon.engine.Event (from re-export)
      2. babylon.engine.event_bus.Event (from original)

      This creates duplicate cross-reference warnings.

    decision: |
      Configure Sphinx, don't restructure Python code.

      Key configuration in docs/conf.py:
      ```python
      # Only document members where they're DEFINED, not IMPORTED
      autosummary_imported_members = False
      ```

      This tells Sphinx: when Event is imported into __init__.py,
      don't document it again there. Document it only in event_bus.py.

    rationale:
      - "Re-exports are intentional API design, not a mistake"
      - "Same object in memory: Event1 is Event2 → True"
      - "Users expect `from babylon.engine import Event`"
      - "Flattening __init__.py would break imports across codebase"
      - "Tool should conform to codebase, not vice versa"

    implementation:
      config_changes:
        - "autosummary_imported_members = False"
        - "html_static_path = [] (no custom static files)"
      docstring_fixes:
        - "factories.py: **kwargs RST markup"
        - "scenarios.py: topology diagram formatting"
        - "struggle.py: docstring emphasis"
        - "relationship.py: code block fencing"
        - "formulas.py: block quote formatting"
      module_path_fixes:
        - "docs/api/systems.rst: 6 incorrect module paths"
      exports_added:
        - "SolidaritySystem to __init__.py"
        - "TerritorySystem to __init__.py"

    consequences:
      positive:
        - "Zero Sphinx warnings"
        - "Python API surface unchanged"
        - "Import patterns preserved"
        - "CI now fails on doc warnings (-W flag)"
      negative:
        - "Must understand autosummary behavior for future docs"
        - "Some classes only documented in submodule docs"

    related_mantras:
      - "codebase_is_truth"
      - "differentiate_structural_issues"
      - "docstrings_enable_sphinx"

  ADR023_python_reexport_pattern:
    status: "accepted"
    date: "2025-12-11"
    title: "Python Re-Export Pattern in __init__.py is Intentional"
    context: |
      During Sphinx warning investigation, question arose:
      "Is the re-export pattern a ticking time bomb or just a Sphinx issue?"

      Re-export pattern in question:
      ```python
      # babylon/engine/__init__.py
      from babylon.engine.event_bus import Event
      from babylon.engine.simulation_engine import SimulationEngine
      __all__ = ["Event", "SimulationEngine", ...]
      ```

      Verified with Python runtime:
      ```python
      from babylon.engine import Event as Event1
      from babylon.engine.event_bus import Event as Event2

      Event1 is Event2  # True
      id(Event1) == id(Event2)  # True
      Event1.__module__  # 'babylon.engine.event_bus'
      ```

    decision: |
      KEEP the re-export pattern. This is standard Python API design.

      The re-export does NOT create copies. Python's import system
      ensures there is exactly ONE object in memory. The __init__.py
      re-export simply provides an alternative import path.

      Benefits:
      1. Cleaner imports: `from babylon.engine import Event`
      2. API stability: internal reorganization doesn't break imports
      3. Intentional public surface: __all__ declares what's "public"
      4. Industry standard: pandas, requests, pydantic all do this

    rationale:
      - "This is not duplication - it's aliasing"
      - "Same id(), same __module__, same object"
      - "Standard Python practice for public APIs"
      - "Removing re-exports would break existing imports"
      - "Sphinx issue is tooling, not architecture"

    verification_performed:
      - "Runtime object identity check (is operator)"
      - "Memory address check (id function)"
      - "Module attribute verification (__module__)"
      - "Major library pattern analysis"

    consequences:
      positive:
        - "Clean public API surface"
        - "Internal refactoring flexibility"
        - "Industry-standard pattern"
      negative:
        - "Sphinx needs configuration (ADR022)"
        - "Two import paths exist (may confuse new devs)"

    related_mantras:
      - "codebase_is_truth"
      - "differentiate_structural_issues"

  ADR024_documentation_system:
    status: "accepted"
    date: "2025-12-11"
    title: "Sphinx Documentation with RST and Autodoc"
    context: |
      Babylon needs documentation for:
      1. Human developers (API reference, guides)
      2. AI assistants (machine-readable context)
      3. Theoretical reference (MLM-TW concepts)

      Multiple documentation approaches evaluated.

    decision: |
      Tiered documentation system:

      **Tier 1: Sphinx (Human-Readable)**
      - Location: docs/
      - Format: RST with MyST for Markdown
      - Generation: Autodoc from docstrings
      - Output: HTML (local), potentially ReadTheDocs
      - CI: Build with -W (warnings fatal)

      **Tier 2: AI Docs (Machine-Readable)**
      - Location: ai-docs/
      - Format: YAML
      - Purpose: Context for AI assistants
      - Files: architecture, decisions, ontology, patterns, anti-patterns
      - Tokens: Optimized for context windows

      **Tier 3: Brainstorm (Ideas)**
      - Location: brainstorm/
      - Format: Markdown
      - Purpose: Design exploration, deferred ideas
      - Status: Not documentation, working notes

      **Docstring Standards**:
      - All public classes/functions: docstrings required
      - Format: RST-compatible (Sphinx autodoc)
      - Examples: Should pass doctest
      - Cross-references: :class:`Name`, :func:`name`

    rationale:
      - "Sphinx is Python standard, well-maintained"
      - "Autodoc keeps docs in sync with code"
      - "AI docs are token-efficient for context windows"
      - "Separation prevents docs from becoming stale"
      - "CI enforcement ensures docs build correctly"

    files:
      sphinx:
        - "docs/conf.py"
        - "docs/index.rst"
        - "docs/api/*.rst"
        - "docs/concepts/*.rst"
      ai_docs:
        - "ai-docs/*.yaml"
        - "ai-docs/README.md"
      brainstorm:
        - "brainstorm/mechanics/*.md"
        - "brainstorm/deferred-ideas.md"

    ci_integration:
      job: "docs"
      commands:
        - "poetry run pytest --doctest-modules src/babylon/systems/formulas.py"
        - "cd docs && poetry run sphinx-build -W -b html . _build/html"
      failure_mode: "Blocks merge"

    consequences:
      positive:
        - "Single source of truth (docstrings)"
        - "CI catches documentation issues"
        - "AI docs optimized for AI context"
        - "Clear separation of concerns"
      negative:
        - "Three documentation locations to maintain"
        - "RST formatting has learning curve"
        - "Docstrings must be kept Sphinx-compatible"

    related_mantras:
      - "docstrings_enable_sphinx"
      - "explain_why_not_what"

# =============================================================================
# REJECTED DECISIONS
# =============================================================================

rejected:

  RDR001_postgres:
    date: "2024-12-07"
    proposal: "Use PostgreSQL for persistence"
    rejection_reason: "Overkill for local single-player game"
    replaced_by: "ADR002_sqlite_over_postgres"

  RDR002_toml_data:
    date: "2024-12-07"
    proposal: "Use TOML for game data"
    rejection_reason: |
      JSON has better schema validation ecosystem.
      TOML better for config, JSON better for data.
    note: "TOML may still be used for configuration files"
