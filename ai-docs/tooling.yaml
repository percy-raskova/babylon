# Babylon Tooling Configuration
# How tools are configured and why

meta:
  version: "1.2.0"
  updated: "2025-12-26"
  purpose: "Document tooling decisions to avoid re-investigation"

# =============================================================================
# CI/CD TOOLING
# =============================================================================

cicd:
  philosophy: "Block on correctness, not style"
  governance: "Benevolent Dictator model - see ci-workflow.yaml for full details"

  github_actions:
    ci_yml:
      file: ".github/workflows/ci.yml"
      triggers:
        push: ["main", "dev"]
        pull_request: ["main", "dev"]
      jobs:
        ci:
          name: "Lint, Type Check & Test"
          blocks_merge: true
          steps:
            - "poetry run ruff check ."
            - "poetry run mypy src"
            - "poetry run pytest -m 'not ai' --tb=short -q"
        docs:
          name: "Documentation Build"
          blocks_merge: true
          steps:
            - "poetry run pytest --doctest-modules src/babylon/systems/formulas.py -v"
            - "cd docs && poetry run sphinx-build -b html . _build/html"
          note: "No -W flag - warnings allowed in CI, strict mode available locally"
        style:
          name: "Style Check (Informational)"
          blocks_merge: false
          continue_on_error: true
          steps:
            - "poetry run ruff format --check ."

    extended_analysis_yml:
      file: ".github/workflows/extended-analysis.yml"
      triggers:
        - "release (published)"
        - "schedule (weekly Sunday midnight)"
        - "workflow_dispatch (manual)"
      jobs:
        extended_tests:
          matrix: ["3.12", "3.13"]
          purpose: "Python version compatibility"
        parameter_analysis:
          commands:
            - "mise run analyze-trace"
            - "mise run analyze-sweep"
          artifacts: "results/"
        ai_evaluation:
          condition: "release only"
          command: "poetry run pytest -m 'ai' --tb=short -q"

  pr_template:
    file: ".github/PULL_REQUEST_TEMPLATE.md"
    tone: "Beginner-friendly, welcoming"
    key_message: "Checklist is a guide, not a gate"

  pre_commit:
    file: ".pre-commit-config.yaml"
    purpose: "Local style enforcement (not CI)"
    hooks:
      - "ruff (lint)"
      - "ruff (format)"
      - "mypy (typecheck)"
      - "pytest (fast tests)"
      - "yamllint"
      - "commitizen (commit message)"

# =============================================================================
# DOCUMENTATION TOOLING
# =============================================================================

documentation:

  sphinx:
    config_file: "docs/conf.py"
    key_settings:
      autosummary_imported_members: false
      reason: "Prevents duplicate documentation of re-exported classes"

      html_static_path: "[]"
      reason: "No custom static files, avoids missing directory warning"

      autodoc_default_options:
        members: true
        undoc_members: true
        show_inheritance: true

    extensions:
      - "sphinx.ext.autodoc"
      - "sphinx.ext.autosummary"
      - "sphinx.ext.viewcode"
      - "sphinx.ext.intersphinx"
      - "sphinx.ext.doctest"
      - "sphinx.ext.todo"
      - "sphinx.ext.coverage"
      - "sphinx_autodoc_typehints"
      - "myst_parser"
      - "sphinxcontrib.mermaid"  # Diagram support for flowcharts

    ci_flags:
      b_html: "HTML builder"
      note: "-W (warnings as errors) removed from CI, available via mise run docs-strict"

    suppress_warnings:
      config_location: "docs/conf.py"
      suppressed:
        - "autodoc"
        - "autodoc.import_object"
        - "ref.python"
        - "ref.ref"
        - "myst.xref_missing"
        - "docutils"
      reason: "Pydantic model re-exports cause unavoidable duplicate object warnings"

  docstring_format:
    style: "RST (reStructuredText)"
    reason: "Sphinx autodoc native format"
    rules:
      - "Use :: for code blocks, not markdown ```"
      - "Use *italic* and **bold** with proper spacing"
      - "Use :class:`ClassName` for cross-references"
      - "Blank line before and after code blocks"
      - "Examples should pass doctest"

    examples:
      good: |
        """Calculate imperial rent.

        The formula applies Marxist value theory to compute
        the surplus extracted via unequal exchange.

        Args:
            wages: Currency amount paid to workers
            value: Currency amount of value produced

        Returns:
            Imperial rent (Phi) extracted

        Example:
            >>> calculate_imperial_rent(wages=100.0, value=80.0)
            20.0

        See Also:
            :func:`calculate_exploitation_rate`
        """
      bad: |
        """
        Calculate imperial rent.
        Uses `code` markdown style - WRONG for RST
        Returns the **bold without spaces**error
        """

# =============================================================================
# LINTING TOOLING
# =============================================================================

linting:

  ruff:
    config_file: "pyproject.toml"
    version: "^0.14.0"
    important: "Must match pre-commit version exactly"
    settings:
      line_length: 100
      target_version: "py312"
      select:
        - "E"   # pycodestyle errors
        - "W"   # pycodestyle warnings
        - "F"   # pyflakes
        - "I"   # isort
        - "B"   # bugbear
        - "C4"  # comprehensions
        - "UP"  # pyupgrade
        - "ARG" # unused arguments
        - "SIM" # simplify
      ignore:
        - "E501"  # line length (handled by formatter)

  mypy:
    config_file: "pyproject.toml"
    mode: "strict"
    settings:
      python_version: "3.12"
      strict: true
      disallow_untyped_defs: true
      warn_return_any: true
      show_error_codes: true
      exclude: ["tests/", "build/", "dist/", "assets/music/"]

  yamllint:
    config_file: ".yamllint.yaml"
    settings:
      line_length: 120
      indentation: 2
      truthy_allowed: ["true", "false", "on", "off", "yes", "no"]
    ignore:
      - ".venv/"
      - ".git/"
      - "node_modules/"
      - "brainstorm/"
      - "docs/character_sheets/"
      - "ai-docs/"
    reason_for_ai_docs_ignore: "Prose with long lines, not infrastructure config"

# =============================================================================
# CODE COMPLEXITY ANALYSIS
# =============================================================================

code_complexity:
  philosophy: "Guard against unmaintainable code in AI-heavy development (80% AI-written)"

  rationale: |
    When AI writes 80% of the code, complexity guardrails aren't over-engineering—
    they're essential. Without gates, AI can produce sprawling functions that pass
    tests but become debugging nightmares. Cyclomatic complexity catches these
    before they merge.

  metrics:
    cyclomatic_complexity:
      status: "primary"
      tool: "radon"
      description: "Count of independent code paths through a function"
      why_it_matters: |
        High CC = hard to test, hard to understand, bug-prone.
        Mathematical formulas should be pure functions with low CC.
        Orchestration code may legitimately be higher, but has limits.
      grading_scale:
        A: "1-5 (simple, low risk)"
        B: "6-10 (moderate complexity, acceptable)"
        C: "11-20 (complex, should consider refactoring)"
        D: "21-30 (very complex, needs attention)"
        E: "31-40 (untestable, urgent refactoring)"
        F: "41+ (error-prone, critical issue)"

    maintainability_index:
      status: "excluded"
      reason: |
        MI is a composite score combining CC, Halstead volume, and LOC.
        Problem: Babylon's mandatory Sphinx docstrings artificially inflate MI.
        A function could have good MI but still be poorly structured.
        CC alone is sufficient with our existing 100-line function rule.

    halstead_metrics:
      status: "excluded"
      reason: "Too academic, high noise-to-signal ratio. Rarely actionable."

    raw_loc:
      status: "excluded"
      reason: "Already enforced via CLAUDE.md 100-line function rule."

  tools:
    radon:
      purpose: "Detailed complexity reporting"
      package: "radon"
      usage: "poetry run radon cc src/ -a -s"
      output: "Per-function CC grades, averages, summary"
      when_used: "All CI runs (informational)"

    xenon:
      purpose: "Pass/fail complexity gating"
      package: "xenon"
      usage: "poetry run xenon --max-absolute C src/"
      output: "Exit code 0 (pass) or 1 (fail)"
      when_used: "Blocking gate on main branch only"

    ruff_c901:
      purpose: "Fast local feedback via pre-commit"
      package: "ruff (built-in mccabe rule)"
      usage: "Enabled in ruff config with max-complexity = 15"
      when_used: "Pre-commit hook (immediate developer feedback)"

  thresholds:
    warning_level:
      cc_value: 10
      grade: "B/C boundary"
      enforcement: "Informational only, shown in CI logs"

    blocking_level:
      cc_value: 15
      grade: "Stricter than Google (10), pragmatic for existing codebase"
      enforcement: "Blocks merge to main via Xenon"
      rationale: |
        Why 15 instead of 10?
        - Codebase already exists with potential complexity
        - Starting strict risks false positives that frustrate adoption
        - Can tighten to 10 after baselining current state
        - Industry median is 10-15, this is reasonable middle ground

  workflow:
    feature_branches:
      complexity_check: "none"
      rationale: "Active development is messy, allow iteration"

    pull_requests_to_dev:
      complexity_check: "radon report (informational)"
      blocks_merge: false
      rationale: "Visibility without friction, developers see the stats"

    pull_requests_to_main:
      complexity_check: "xenon gate"
      blocks_merge: true
      rationale: "Quality gate before stable releases"

    push_to_main:
      complexity_check: "xenon gate"
      blocks_merge: true
      rationale: "Redundant safety for direct pushes (BD only)"

  ci_integration:
    all_branches:
      step_name: "Code Complexity Report"
      command: "poetry run radon cc src/ -a -s --show-complexity"
      continue_on_error: true
      purpose: "Visibility in CI logs"

    main_only:
      step_name: "Code Complexity Gate"
      command: "poetry run xenon --max-absolute C src/"
      continue_on_error: false
      purpose: "Block merge if any function exceeds CC 15"

  mise_tasks:
    complexity:
      command: "poetry run radon cc src/ -a -s --show-complexity"
      description: "Show full complexity report"

    complexity_check:
      command: "poetry run xenon --max-absolute C src/"
      description: "Run xenon gate (test before main PR)"

  pre_commit_integration:
    rule: "C901"
    config_location: "pyproject.toml [tool.ruff.lint]"
    setting: "max-complexity = 15"
    behavior: "Fails pre-commit if any function exceeds CC 15"
    rationale: "Immediate local feedback before push"

  scope:
    included:
      - "src/babylon/**/*.py"
    excluded:
      paths:
        - "tests/**/*.py"
      reason: "Test functions are intentionally verbose for clarity"

  known_patterns:
    acceptable_high_complexity:
      examples:
        - "State machines (explicit case handling)"
        - "Parsers with many grammar rules"
        - "Factory functions with many entity types"
      mitigation: |
        1. Refactor into smaller functions where possible
        2. If truly unavoidable, document WHY in PR description
        3. Xenon allows --exclude for specific modules (use sparingly)

    common_causes_of_high_cc:
      - "Nested if/elif chains (consider lookup tables)"
      - "Multiple early returns (consolidate conditions)"
      - "Try/except with many exception types (use base classes)"
      - "Boolean flag parameters (split into separate functions)"

  future_enhancements:
    wily:
      description: "Track complexity OVER TIME (detect regressions)"
      status: "Not implemented yet"
      use_case: "Could catch 'this PR increased average CC by 2 points'"

    path_based_thresholds:
      description: "Different CC limits for different code areas"
      status: "Not implemented yet"
      example: |
        - formulas.py: max CC 10 (pure math, no excuses)
        - engine/systems/*.py: max CC 15 (some coordination OK)

    cognitive_complexity:
      description: "Alternative to CC that weights nested structures higher"
      status: "Blocked - Ruff doesn't support yet"
      notes: "When Ruff adds cognitive complexity, consider switching"

# =============================================================================
# MUTATION TESTING
# =============================================================================

mutation_testing:
  philosophy: "Coverage lies - mutation testing proves tests actually catch bugs"

  rationale: |
    You can have 100% line coverage but still miss bugs. Mutation testing
    makes small changes to code (mutations) and verifies tests catch them.
    If tests pass with mutated code, you have a test gap.

  tool: "mutmut"
  package: "mutmut"
  config_file: "pyproject.toml [tool.mutmut]"

  how_it_works:
    - "Makes small changes (mutations) to source code"
    - "Runs test suite against each mutation"
    - "If tests still pass → mutant 'survived' (test gap!)"
    - "If tests fail → mutant 'killed' (tests are effective)"

  example_mutations:
    - "Change > to >= (boundary conditions)"
    - "Change + to - (arithmetic operators)"
    - "Replace return value with constant"
    - "Remove statement entirely"

  ci_integration:
    trigger: "PRs to main only"
    scope: "src/babylon/systems/formulas.py (critical pure math)"
    behavior: "Blocks merge if mutants survive"
    rationale: |
      formulas.py contains the mathematical core of the simulation.
      These functions MUST be correctly tested. Mutation testing
      ensures tests catch off-by-one errors, boundary issues, etc.

  mise_tasks:
    mutate:
      command: "poetry run mutmut run --paths-to-mutate=src/babylon/systems/formulas.py"
      description: "Run mutation testing on core formulas (~15-30 min)"

    mutate_critical:
      command: "poetry run mutmut run --paths-to-mutate=..."
      description: "Run mutation testing on critical engine systems"

    mutate_results:
      command: "poetry run mutmut results"
      description: "Show mutation testing results summary"

    mutate_show:
      command: "poetry run mutmut show <id>"
      description: "Show specific mutant details"

    mutate_html:
      command: "poetry run mutmut html"
      description: "Generate HTML report"

  workflow:
    periodic_audit:
      - "Run mise run mutate periodically (weekly or per-release)"
      - "Review surviving mutants with mise run mutate-results"
      - "Investigate specific mutants with mise run mutate-show <id>"
      - "Write tests to kill survivors"

    pr_to_main:
      - "CI automatically runs mutation testing"
      - "PR blocked if any mutants survive in formulas.py"
      - "Developer must add tests to kill survivors before merge"

  time_cost:
    formulas_py: "~15-30 minutes (200-400 mutations)"
    full_codebase: "Hours (not recommended for CI)"

  why_only_main:
    - "Mutation testing is slow (full test run per mutation)"
    - "PRs to main are rare (BD merges only)"
    - "Main is stable release branch - quality gate justified"
    - "Dev branches move fast - don't want to block velocity"

# =============================================================================
# PYTHON TOOLING
# =============================================================================

python:

  poetry:
    config_file: "pyproject.toml"
    version: "1.8.4"
    dependency_groups:
      main: "Core runtime dependencies"
      dev: "Testing, linting, type checking"
      docs: "Sphinx and documentation tools"

  pytest:
    config_file: "pyproject.toml"
    markers:
      unit: "Fast unit tests for isolated components"
      math: "Deterministic mathematical formulas"
      ledger: "Economic/Political state tests"
      topology: "Graph/Network operations"
      integration: "Database/ChromaDB tests"
      ai: "AI/RAG evaluation tests (slow, non-deterministic)"
    asyncio_mode: "strict"

  mise:
    config_file: "mise.toml"
    purpose: "Task runner"
    key_tasks:
      ci: "Quick CI: lint + format + typecheck + test-fast"
      test: "Run all non-AI tests"
      test_fast: "Fast math/engine tests only"
      typecheck: "MyPy strict mode"
      docs_live: "Live-reload documentation server"
      analyze_trace: "Single sim with full time-series CSV"
      analyze_sweep: "Parameter sweep with summary metrics"
      "analyze:matrix": "2D parameter landscape grid search"
      "qa:audit": "Generate simulation health report"
      tune_agent: "Optuna-based Bayesian optimization (PLANNED)"

# =============================================================================
# PARAMETER TUNING TOOLING
# =============================================================================

parameter_tuning:
  philosophy: "Bayesian Optimization over Grid Search for computational efficiency"
  migration_date: "2025-12-30"

  primary_tool:
    name: "tune_agent.py"
    location: "tools/tune_agent.py"
    status: "PLANNED"
    description: |
      Autonomous tuning agent using Optuna to optimize simulation parameters
      against defined objective functions. Uses TPE (Tree-structured Parzen
      Estimator) for sample-efficient optimization and Hyperband pruning to
      kill non-viable simulations early.

    algorithm: "TPE (Tree-structured Parzen Estimator)"
    pruning: "Hyperband (Successive Halving)"

    capabilities:
      - "Multi-objective optimization (stability + complexity)"
      - "Early termination of failing simulations"
      - "Parameter importance analysis"
      - "Pareto front visualization via Optuna Dashboard"

    usage: |
      # Run optimization study
      poetry run python tools/tune_agent.py --n-trials 200 --study-name imperial_circuit

      # Resume existing study
      poetry run python tools/tune_agent.py --study-name imperial_circuit --resume

      # Launch Optuna Dashboard
      optuna-dashboard sqlite:///optuna.db

    objective_function: |
      Maximize: w1*ticks_survived + w2*rent_pool_health + w3*consciousness_variance
      Subject to: No Comprador death before tick 10 (pruning constraint)

  legacy_tools:
    - name: "tune_parameters.py"
      location: "tools/tune_parameters.py"
      status: "LEGACY - Manual Inspection Only"
      description: |
        Original parameter sweeping tool. Useful for quick manual inspection
        but inefficient for systematic optimization. Retained for debugging
        and understanding individual parameter effects.
      recommendation: "Use for one-off explorations, not systematic tuning"

    - name: "parameter_analysis.py"
      location: "tools/parameter_analysis.py"
      status: "ACTIVE - Trace/Sweep Commands"
      description: |
        Detailed time-series analysis tool. trace command produces full
        CSV of all entity states per tick. sweep command produces summary
        statistics. Still valuable for deep-dive analysis after Optuna
        identifies interesting parameter regions.
      recommendation: "Use for detailed analysis of Optuna-identified optima"

    - name: "landscape_analysis.py"
      location: "tools/landscape_analysis.py"
      status: "ACTIVE - 2D Visualization"
      description: |
        2D parameter grid search for visualizing interaction effects.
        Produces matrix CSV suitable for heatmap visualization. Useful
        for understanding parameter interactions in reduced dimensions.
      recommendation: "Use for visualizing 2D slices of parameter space"

  dependencies:
    optuna:
      package: "optuna"
      version: "^3.6"
      purpose: "Bayesian optimization framework"
      group: "dev"

    optuna_dashboard:
      package: "optuna-dashboard"
      version: "^0.15"
      purpose: "Web-based visualization of optimization studies"
      group: "dev"
      usage: "optuna-dashboard sqlite:///optuna.db"

  storage:
    database: "sqlite:///optuna.db"
    location: "Project root (gitignored)"
    purpose: "Persistent storage of optimization studies for resumption"

  mise_tasks_planned:
    tune:
      command: "poetry run python tools/tune_agent.py"
      description: "Run Optuna optimization agent"

    tune_dashboard:
      command: "optuna-dashboard sqlite:///optuna.db"
      description: "Launch Optuna Dashboard for study analysis"

    tune_best:
      command: "poetry run python tools/tune_agent.py --show-best"
      description: "Show best parameters from completed study"

# =============================================================================
# TESTING INFRASTRUCTURE
# =============================================================================

testing_infrastructure:
  description: "Consolidated testing tools from Operation Clean Slate refactor (Phase 2.5)"
  added: "2025-12-12"

  domain_factory:
    location: "tests/factories/domain.py"
    status: ACTIVE
    test_count: 31
    purpose: "Single source of truth for creating test entities"
    key_methods:
      - "create_worker(): SocialClass with periphery proletariat defaults"
      - "create_owner(): SocialClass with core bourgeoisie defaults"
      - "create_relationship(): Relationship edge with exploitation defaults"
      - "create_world_state(): Complete WorldState for simulation tests"
    usage: |
      from tests.factories import DomainFactory
      factory = DomainFactory()
      worker = factory.create_worker(wealth=100.0)  # Override only what matters
      state = factory.create_world_state(entities={"C001": worker})
    replaces: "Manual dictionary creation, scattered fixture definitions"

  babylon_assert:
    location: "tests/assertions.py"
    status: ACTIVE
    test_count: 55
    purpose: "Fluent assertion library for domain-expressive test verification"
    key_classes:
      - "Assert: Entry point for WorldState assertions"
      - "EntityAssert: SocialClass assertions (wealth, ideology, survival)"
      - "RelationshipAssert: Relationship assertions (tension, value_flow)"
    usage: |
      from tests.assertions import Assert
      Assert(new_state).entity("C001").is_poorer_than(previous_state)
      Assert(state).relationship("C001", "C002").has_tension_increased(previous_state)
    replaces: "Raw assert statements with manual calculations"

  mock_metrics_collector:
    location: "tests/mocks/metrics_collector.py"
    status: ACTIVE
    test_count: 8
    purpose: "Pure spy pattern for metrics recording (no calculations)"
    pattern: "Dumb Spy - records calls without computing statistics"
    key_methods:
      - "record_cache_event(level, hit): Track cache hits/misses"
      - "record_metric(name, value, context): Generic metric recording"
      - "get_recorded_data(): Return raw recorded values"
      - "get_protocol_calls(): List all protocol method calls"
      - "clear(): Reset all metrics"
    usage: |
      from tests.mocks import MockMetricsCollector
      collector = MockMetricsCollector()
      collector.record_cache_event("L1", hit=True)
      data = collector.get_recorded_data()
      assert data["cache_hits"]["L1"] == 1
    note: "Self-contained tests embedded in module (lines 328-456)"

  metrics_collector_protocol:
    location: "src/babylon/metrics/interfaces.py"
    status: ACTIVE
    purpose: "Protocol interface ensuring mocks and real collectors share identical API"
    key_methods:
      - "record(name, value, tags, metadata): Record metric value"
      - "increment(name, value): Increment counter"
      - "gauge(name, value): Set gauge value"
      - "time(name) -> ContextManager: Time operations"
      - "summary() -> dict: Get aggregated metrics"
      - "clear(): Reset all metrics"

# =============================================================================
# KNOWN ISSUES AND WORKAROUNDS
# =============================================================================

known_issues:

  sphinx_duplicate_warnings:
    problem: "Classes re-exported in __init__.py documented twice"
    solution: "autosummary_imported_members = False"
    reference: "ADR022_sphinx_autosummary_config"

  ruff_version_mismatch:
    problem: "Different ruff versions in Poetry vs pre-commit cause format differences"
    solution: "Always keep versions in sync: pyproject.toml and .pre-commit-config.yaml"
    ci_impact: "Style job fails if versions drift"

  yamllint_prose:
    problem: "Prose files in ai-docs/ exceed line length limits"
    solution: "Add ai-docs/ to yamllint ignore in .yamllint.yaml"
    rationale: "Prose is not infrastructure config, line limits don't apply"

  python_313_test_flakiness:
    problem: "Some tests fail on Python 3.13 due to timing differences"
    example: "timestamp collision in checkpoint tests"
    solution: "Add time.sleep(0.01) between time-sensitive operations"
    note: "Python 3.13 tests run in extended-analysis, not main CI"
